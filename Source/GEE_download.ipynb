{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import geemap\n",
    "import ee\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from pyproj import Proj, transform\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization successful\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    ee.Initialize()\n",
    "    print(\"Initialization successful\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: authentication needed\")\n",
    "    ee.Authenticate()\n",
    "    ee.Initialize()\n",
    "    print(\"Initialization successful\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TS function\n",
    "def dailyNBARmaskFunc(img):\n",
    "    qa = img.select('BRDF_Albedo_Band_Mandatory_Quality_Band1')\n",
    "    qa2 = img.select('BRDF_Albedo_Band_Mandatory_Quality_Band2')\n",
    "    Quality = bitwiseExtract(qa, 0)\n",
    "    Qualityb2 = bitwiseExtract(qa2, 0)\n",
    "    mask = Quality.eq(0)\\\n",
    "        .And(Qualityb2.eq(0))\\\n",
    "\n",
    "\n",
    "    maskedImage = img.updateMask(mask)\n",
    "\n",
    "    return maskedImage\n",
    "\n",
    "def dailyNBARNDVI(img):\n",
    "    ndvi = img.normalizedDifference(['Nadir_Reflectance_Band2', 'Nadir_Reflectance_Band1']).rename('NDVI').set('system:time_start', img.get('system:time_start'))\n",
    "    return img.addBands(ndvi)\n",
    "\n",
    "def bitwiseExtract(value, fromBit, toBit=None):\n",
    "    '''\n",
    "    https://gis.stackexchange.com/questions/349371/creating-cloud-free-images-out-of-a-mod09a1-modis-image-in-gee/349401#349401\n",
    "    '''\n",
    "    if toBit == None:\n",
    "        toBit = fromBit\n",
    "    maskSize = ee.Number(1).add(toBit).subtract(fromBit)\n",
    "    mask = ee.Number(1).leftShift(maskSize).subtract(1)\n",
    "    return value.rightShift(fromBit).bitwiseAnd(mask)\n",
    "\n",
    "def Getroi(img):\n",
    "    \n",
    "    maskedImage = img.clip(roi)\n",
    "    \n",
    "    return maskedImage\n",
    "def rescale(image):\n",
    "    date = image.get('system:time_start')\n",
    "    return image.multiply(scale_factor).set('system:time_start', date)\n",
    "\n",
    "def createTS(image):\n",
    "    date = image.get('system:time_start')\n",
    "    value = image.reduceRegion(reducer=ee.Reducer.mean(), geometry=roi).get(var)\n",
    "    std = image.reduceRegion(reducer=ee.Reducer.stdDev(), geometry=roi).get(var)\n",
    "    ft = ee.Feature(None, {'date': ee.Date(date).format('Y/M/d'), var: value, 'STD': std})\n",
    "    return ft\n",
    "\n",
    "def TS_to_pandas(TS):\n",
    "    dump = TS.getInfo()\n",
    "    fts = dump['features']\n",
    "    out_vals = np.empty((len(fts)))\n",
    "    out_dates = []\n",
    "    out_std = np.empty((len(fts)))\n",
    "    \n",
    "    for i, f in enumerate(fts):\n",
    "        props = f['properties']\n",
    "        date = props['date']\n",
    "        val = props[var]\n",
    "        std = props['STD']\n",
    "        out_vals[i] = val\n",
    "        out_std[i] = std\n",
    "        out_dates.append(pd.Timestamp(date))\n",
    "    \n",
    "    df = pd.DataFrame({'mean' : out_vals, 'std' : out_std}, index=out_dates)\n",
    "    return df\n",
    "\n",
    "\n",
    "#GEE interpolation cloud comp\n",
    "#credit https://spatialthoughts.com/2021/11/08/temporal-interpolation-gee/\n",
    "def interpolate(image):\n",
    "    image = ee.Image(image)\n",
    "    date = image.get('system:time_start')\n",
    "    beforeImages = ee.List(image.get('before'))\n",
    "    beforeMosaic = ee.ImageCollection.fromImages(beforeImages).mosaic()\n",
    "    afterImages = ee.List(image.get('after'))\n",
    "    afterMosaic = ee.ImageCollection.fromImages(afterImages).mosaic()\n",
    "    \n",
    "    t1 = beforeMosaic.select('timestamp').rename('t1')\n",
    "    t2 = afterMosaic.select('timestamp').rename('t2')\n",
    "    t = image.metadata('system:time_start').rename('t')\n",
    "    tImage = ee.Image.cat([t1, t2, t])\n",
    "    timeRatio = tImage.expression('(t - t1) / (t2 - t1)', {'t': tImage.select('t'),'t1': tImage.select('t1'),'t2': tImage.select('t2')})\n",
    "\n",
    "    interpolated = beforeMosaic.add((afterMosaic.subtract(beforeMosaic).multiply(timeRatio)))\n",
    "    result = image.unmask(interpolated)\n",
    "    return result.copyProperties(image, ['system:time_start'])\n",
    "\n",
    "def timeImage(image):\n",
    "    tI = image.metadata('system:time_start').rename('timestamp')\n",
    "    timeImageMasked = tI.updateMask(image.mask().select(14))\n",
    "    return image.select('NDVI').addBands(timeImageMasked)\n",
    "\n",
    "def getsiteNDVI(roiall, NDVICollection, siteID, y_start, y_end, n):\n",
    "    coords = roiall.getInfo()['features'][siteID]['geometry']['coordinates']\n",
    "    global roi\n",
    "    roi = ee.Geometry.MultiPolygon(coords)\n",
    "\n",
    "    NDVI_flt = NDVICollection.filter(ee.Filter.date(y_start, y_end))\n",
    "    NDVImasked = NDVI_flt.map(dailyNBARmaskFunc)\n",
    "    NDVI_roimasked = NDVImasked.map(Getroi)\n",
    "\n",
    "    NDVI_rescale = NDVI_roimasked.map(rescale)\n",
    "    NDVI_rescale = NDVI_rescale.map(dailyNBARNDVI)\n",
    "    #pixel based interpolation\n",
    "    days = n\n",
    "\n",
    "    millis = ee.Number(days).multiply(1000*60*60*24)\n",
    "\n",
    "    NDVI_rescale_t = NDVI_rescale.map(timeImage)\n",
    "\n",
    "    maxDiffFilter = ee.Filter.maxDifference(**{'difference': millis,'leftField': 'system:time_start','rightField': 'system:time_start'})\n",
    "\n",
    "    lessEqFilter = ee.Filter.lessThanOrEquals(**{'leftField': 'system:time_start','rightField': 'system:time_start'})\n",
    "\n",
    "    greaterEqFilter = ee.Filter.greaterThanOrEquals(**{'leftField': 'system:time_start','rightField': 'system:time_start'})\n",
    "\n",
    "    filter1 = ee.Filter.And(maxDiffFilter, lessEqFilter)\n",
    "\n",
    "    join1 = ee.Join.saveAll(**{'matchesKey': 'after','ordering': 'system:time_start','ascending': False})\n",
    "\n",
    "    join1Result = join1.apply(**{'primary': NDVI_rescale_t,'secondary': NDVI_rescale_t,'condition': filter1})\n",
    "\n",
    "    filter2 = ee.Filter.And(maxDiffFilter, greaterEqFilter)\n",
    "\n",
    "    join2 = ee.Join.saveAll(**{'matchesKey': 'before','ordering': 'system:time_start','ascending': True})\n",
    "\n",
    "    join2Result = join2.apply(**{'primary':join1Result, 'secondary': join1Result, 'condition': filter2})\n",
    "\n",
    "    # Map the interpolation function over the image collection\n",
    "    interpolated_collection = ee.ImageCollection(join2Result.map(interpolate))\n",
    "    #interpolated_collection_ndvi =interpolated_collection.select(['NDVI']).map(lambda img:img.multiply(1).copyProperties(img, **{'properties':['system:time_start', 'system:index']}))\n",
    "    TS = interpolated_collection.map(createTS)\n",
    "    NDVI_ts_int = TS_to_pandas(TS)\n",
    "    return NDVI_ts_int\n",
    "\n",
    "#Season functions\n",
    "\n",
    "def snowdf(roiall,snow, siteID):\n",
    "    coords = roiall.getInfo()['features'][siteID]['geometry']['coordinates']\n",
    "    global roi, var\n",
    "    roi = ee.Geometry.MultiPolygon(coords)\n",
    "    var = 'NDSI_Snow_Cover'\n",
    "\n",
    "    snowcover = snow.select('NDSI_Snow_Cover').sort('system:time_start').filterBounds(roi).map(Getroi)\n",
    "    ts=snowcover.map(createTS)\n",
    "    df=TS_to_pandas(ts)\n",
    "    return df\n",
    "\n",
    "def snowmaskFunc(img):\n",
    "    qa = img.select('NDSI_Snow_Cover_Basic_QA')\n",
    "    Quality = bitwiseExtract(qa, 0, 15) \n",
    "    mask = Quality.eq(0).Or(Quality.eq(1))\n",
    "\n",
    "\n",
    "    maskedImage = img.updateMask(mask)\n",
    "\n",
    "    return maskedImage\n",
    "\n",
    "\n",
    "def LSTdf(roiall, LST, siteID):\n",
    "    coords = roiall.getInfo()['features'][siteID]['geometry']['coordinates']\n",
    "    global roi, var\n",
    "    roi = ee.Geometry.MultiPolygon(coords)\n",
    "    var = 'LST_Day_1km'\n",
    "    \n",
    "    LSTC = LST.select(var).sort('system:time_start').filterBounds(roi).map(Getroi)\n",
    "    ts=LSTC.map(createTS)\n",
    "    df=TS_to_pandas(ts)\n",
    "    return df\n",
    "\n",
    "def LSTmaskFunc(img):\n",
    "    qa = img.select('QC_Day')\n",
    "    Quality = bitwiseExtract(qa, 0, 1)\n",
    "    Quality2 = bitwiseExtract(qa, 2, 3) \n",
    "    mask = Quality.eq(0).Or(Quality2.eq(0))\n",
    "\n",
    "    maskedImage = img.updateMask(mask)\n",
    "\n",
    "    return maskedImage\n",
    "\n",
    "def seasonrangeCal(snowts, LSTts, yt):\n",
    "    lastsnow = np.array((snowts.loc[yt]['mean'][:120]>50)).nonzero()[0]\n",
    "    startsnow = np.array((snowts.loc[yt]['mean'][-120:]>50)).nonzero()[0]\n",
    "    spring = np.array((LSTts.loc[yt]['mean'][:120]*0.02-273.15>0)).nonzero()[0]\n",
    "    winter = np.array((LSTts.loc[yt]['mean'][-120:]*0.02-273.15<0)).nonzero()[0]\n",
    "\n",
    "    if len(lastsnow)!=0:\n",
    "        start_season = snowts.loc[yt].index[lastsnow[-1]]\n",
    "    elif len(spring)!=0:\n",
    "        start_season = snowts.loc[yt].index[spring[0]]\n",
    "    else:\n",
    "        start_season = snowts.loc[yt].index[0]\n",
    "    if len(startsnow)!=0:\n",
    "        end_season = snowts.loc[yt].index[-120:][startsnow[0]]\n",
    "    elif len(winter)!=0:\n",
    "        end_season = snowts.loc[yt].index[-120:][winter[0]]\n",
    "    else: \n",
    "        end_season = snowts.loc[yt].index[-1]\n",
    "    return start_season, end_season\n",
    "\n",
    "def Season(roiall, siteID, y_start, y_end):\n",
    "    startL=[]\n",
    "    endL=[]\n",
    "    snow=ee.ImageCollection('MODIS/061/MOD10A1').filter(ee.Filter.date(f'{y_start}-01-01', f'{y_end}-01-01'))\n",
    "    snow=snow.map(snowmaskFunc)\n",
    "    snowts=snowdf(roiall, snow, siteID)\n",
    "\n",
    "    LST=ee.ImageCollection('MODIS/061/MOD11A1').filter(ee.Filter.date(f'{y_start}-01-01', f'{y_end}-01-01'))\n",
    "    LST=LST.map(LSTmaskFunc)\n",
    "    LSTts=LSTdf(roiall, LST, siteID)\n",
    "    Years = np.arange(y_start, y_end, 1)\n",
    "    for y in Years:\n",
    "        yt=f'{y}'\n",
    "        st,ed=seasonrangeCal(snowts, LSTts, yt)\n",
    "        #st,ed=seasonrangeCal(snowts,0 , yt)\n",
    "        startL.append(st)\n",
    "        endL.append(ed)\n",
    "    df=pd.DataFrame(data={'start_season' : startL, 'end_season' : endL}, index=Years)\n",
    "    return df, snowts, LSTts\n",
    "\n",
    "#cdl function\n",
    "def createTScdl(image):\n",
    "    date = image.get('system:time_start')\n",
    "    value = image.reduceRegion(reducer=ee.Reducer.median(), geometry=roi).get('cropland')\n",
    "    ft = ee.Feature(None, {'date': ee.Date(date).format('Y/M/d'), 'mean': value})\n",
    "    return ft\n",
    "\n",
    "def TS_to_pandascdl(TS):\n",
    "    dump = TS.getInfo()\n",
    "    fts = dump['features']\n",
    "    out_vals = np.empty((len(fts)))\n",
    "    out_dates = []\n",
    "    \n",
    "    for i, f in enumerate(fts):\n",
    "        props = f['properties']\n",
    "        date = props['date']\n",
    "        if len(f['properties'])==1:\n",
    "            val = 0\n",
    "        else:\n",
    "            val = props['mean']\n",
    "        out_vals[i] = val\n",
    "        out_dates.append(pd.Timestamp(date))\n",
    "    \n",
    "    df = pd.DataFrame({'crop' : out_vals}, index=out_dates)\n",
    "    return df\n",
    "\n",
    "def cdldf(cdl, siteID):\n",
    "    coords = roiall.getInfo()['features'][siteID]['geometry']['coordinates']\n",
    "    global roi\n",
    "    roi = ee.Geometry.MultiPolygon(coords)\n",
    "    \n",
    "    cropLandcover = cdl.select('cropland').sort('system:time_start').filterBounds(roi).map(Getroi)\n",
    "    ts=cropLandcover.map(createTScdl)\n",
    "    df=TS_to_pandascdl(ts)\n",
    "    df.index=df.index.year\n",
    "    return df\n",
    "\n",
    "#sentinel 1 function\n",
    "def createTSS1(image):\n",
    "    date = image.get('system:time_start')\n",
    "    orb = image.get('relativeOrbitNumber_start')\n",
    "    slice = image.get('sliceNumber')\n",
    "    VVvalue = image.reduceRegion(reducer=ee.Reducer.mean(), geometry=roi).get('VV')\n",
    "    VVmx = image.reduceRegion(reducer=ee.Reducer.max(), geometry=roi).get('VV')\n",
    "    VHvalue = image.reduceRegion(reducer=ee.Reducer.mean(), geometry=roi).get('VH')\n",
    "    VHmx = image.reduceRegion(reducer=ee.Reducer.max(), geometry=roi).get('VH')\n",
    "    ft = ee.Feature(None, {'date': ee.Date(date).format('Y/M/d'), 'VVmean': VVvalue, 'VVmax':VVmx, 'VHmean': VHvalue\n",
    "                           , 'VHmax':VHmx, 'orbit':orb, 'slice':slice})\n",
    "    return ft\n",
    "\n",
    "def TS_to_pandass1(TS):\n",
    "    dump = TS.getInfo()\n",
    "    fts = dump['features']\n",
    "    out_vals = np.empty((len(fts)))\n",
    "    out_vals2 = np.empty((len(fts)))\n",
    "    out_vals3 = np.empty((len(fts)))\n",
    "    out_vals4 = np.empty((len(fts)))\n",
    "    out_dates = []\n",
    "    out_orbits = []\n",
    "    out_slices = []\n",
    "    \n",
    "    for i, f in enumerate(fts):\n",
    "        props = f['properties']\n",
    "        date = props['date']\n",
    "        val = props['VVmean']\n",
    "        out_vals[i] = val\n",
    "        val = props['VVmax']\n",
    "        out_vals2[i] = val\n",
    "        val = props['VHmean']\n",
    "        out_vals3[i] = val\n",
    "        val = props['VHmax']\n",
    "        out_vals4[i] = val\n",
    "        out_dates.append(pd.Timestamp(date))\n",
    "        out_orbits.append(props['orbit'])\n",
    "        out_slices.append(props['slice'])\n",
    "    \n",
    "    df = pd.DataFrame({'VVmean' : out_vals, 'VVmax': out_vals2, 'VHmean' : out_vals3, 'VHmax': out_vals4\n",
    "                       , 'orbit':out_orbits, 'slice':out_slices}, index=out_dates)\n",
    "    return df\n",
    "\n",
    "def s1data(y_start, y_end, siteID, direction = 'Ascending', orbit = None, dataset='FLOAT'):\n",
    "    coords = roiall.getInfo()['features'][siteID]['geometry']['coordinates']\n",
    "    global roi, var\n",
    "    roi = ee.Geometry.MultiPolygon(coords)\n",
    "    if dataset == 'FLOAT':\n",
    "        S1 = ee.ImageCollection('COPERNICUS/S1_GRD_FLOAT') \n",
    "    elif dataset == 'LOG':\n",
    "        S1 = ee.ImageCollection('COPERNICUS/S1_GRD')#log-scaled (in dB)\n",
    "    S1 = S1.filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV'))\\\n",
    "        .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VH'))\\\n",
    "        .filter(ee.Filter.eq('instrumentMode', 'IW'))\\\n",
    "        .filterBounds(roi)\\\n",
    "        .filter(ee.Filter.date(f'{y_start}-01-01', f'{y_end}-01-01'))\n",
    "\n",
    "    if orbit:\n",
    "        S1 = S1.filter(ee.Filter.eq('relativeOrbitNumber_start', orbit))\n",
    "    if direction == 'Ascending':\n",
    "        data = S1.filter(ee.Filter.eq('orbitProperties_pass', 'ASCENDING'))\n",
    "    else:\n",
    "        data = S1.filter(ee.Filter.eq('orbitProperties_pass', 'DESCENDING'))\n",
    "    data = data.map(Getroi)\n",
    "    data = data.map(createTSS1)\n",
    "    df = TS_to_pandass1(data)\n",
    "    return df\n",
    "\n",
    "#CDL consistency function\n",
    "def dd(date):\n",
    "    return ee.Date(date).format('Y-MM-dd')\n",
    "def consist(ar):\n",
    "    if len(ar[1])!=0:\n",
    "        return np.max(ar[1])/np.sum(ar[1])\n",
    "    else:\n",
    "        return 0\n",
    "def warn_mark(i,n):\n",
    "    return f\"{i:04}_{n+2007}\"\n",
    "def warn_site_List(Lst):\n",
    "    return list(map(consist, Lst))\n",
    "def warn_site_years(Lst):\n",
    "    threshold=np.nonzero(np.array(Lst[1])<0.75)[0]\n",
    "    return list(map(lambda x: warn_mark(Lst[0], x), threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NorthDakota loaded\n",
      "Number of sites: 200\n"
     ]
    }
   ],
   "source": [
    "#Case='Idaho'\n",
    "Case='NorthDakota'\n",
    "#Case='Colorado'\n",
    "\n",
    "sites_pth = 'Data/'+Case+'/sites'\n",
    "cdl_pth = 'Data/'+Case+'/cdl'\n",
    "season_pth = 'Data/'+Case+'/season'\n",
    "sentinel1_pth = 'Data/'+Case+'/sentinel1'\n",
    "\n",
    "if not os.path.exists(sites_pth):\n",
    "    os.makedirs(sites_pth)\n",
    "if not os.path.exists(cdl_pth):\n",
    "    os.makedirs(cdl_pth)\n",
    "if not os.path.exists(season_pth):\n",
    "    os.makedirs(season_pth)\n",
    "if not os.path.exists(sentinel1_pth):\n",
    "    os.makedirs(sentinel1_pth)\n",
    "\n",
    "#define variables\n",
    "roi_shp = 'Data/'+Case+'/masklayers/site_mask.shp'\n",
    "roiall = geemap.shp_to_ee(roi_shp)\n",
    "cdl=ee.ImageCollection('USDA/NASS/CDL').filter(ee.Filter.date('2008-01-01', '2023-01-01'))\n",
    "NDVICollection = ee.ImageCollection('MODIS/061/MCD43A4').filter(ee.Filter.date('2000-01-01', '2023-01-01'))\n",
    "var = 'NDVI'\n",
    "scale_factor = 0.0001\n",
    "\n",
    "gdf = gpd.read_file(roi_shp)\n",
    "geometry = gdf['geometry']\n",
    "num_sites=len(geometry)\n",
    "print(Case+' loaded')\n",
    "print(f\"Number of sites: {num_sites}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No missing files in Data/NorthDakota/sites\n",
      "No missing files in Data/NorthDakota/cdl\n",
      "Missing files in Data/NorthDakota/season\n",
      "600\n",
      "Missing files in Data/NorthDakota/sentinel1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for set in [sites_pth, cdl_pth, season_pth, sentinel1_pth]:\n",
    "    file_names = os.listdir(set)\n",
    "    if len(file_names)!=num_sites:\n",
    "        print(f\"Missing files in {set}\")\n",
    "        print(len(file_names))\n",
    "    else:\n",
    "        print(f\"No missing files in {set}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading NorthDakota CDL Consistency: 100%|██████████| 200/200 [1:17:04<00:00, 23.12s/site]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of multicrop cases:  688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "AlldistL=[]\n",
    "for i in tqdm(np.arange(0,num_sites,1),'Downloading '+Case+' CDL Consistency', unit='site'):\n",
    "    coords = roiall.getInfo()['features'][i]['geometry']['coordinates']\n",
    "    roi = ee.Geometry.MultiPolygon(coords)\n",
    "\n",
    "    #create image collection of interested time frame and area and sort by date\n",
    "    im = ((cdl.filterBounds(roi)).map(Getroi)).sort('system:time_start')\n",
    "    #get date list\n",
    "    Acqt=np.sort(im.aggregate_array('system:time_start').map(dd).getInfo())\n",
    "    #convert to list for indexing\n",
    "    imL = im.toList(im.size())\n",
    "\n",
    "    distL=[]\n",
    "    for i in range(Acqt.size):\n",
    "        image=ee.Image(imL.get(i))\n",
    "        imar=ee.ImageCollection(image).getRegion(roi, scale=30).getInfo()\n",
    "        df = pd.DataFrame(imar[1:], columns=imar[0])\n",
    "        #if df['cropland'].notnull().any():\n",
    "        dist=np.array(np.unique(df['cropland'], return_counts=True))\n",
    "        distL.append(dist)\n",
    "    AlldistL.append(distL)\n",
    "site_consistencyL=list(map(warn_site_List, AlldistL))\n",
    "Warn=list(map(warn_site_years, enumerate(site_consistencyL)))\n",
    "\n",
    "count=0\n",
    "for i in Warn:\n",
    "    count+=len(i)\n",
    "print('Total number of multicrop cases: ', count)\n",
    "\n",
    "with open(cdl_pth+\"/CaseID_anom\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(Warn, fp)\n",
    "with open(cdl_pth+\"/ConsistencyPerc\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(site_consistencyL, fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of missing data cases:  0\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for i in range(len(AlldistL)):\n",
    "    lst=AlldistL[i]\n",
    "    for j in range(len(lst)):\n",
    "        ar=lst[j]\n",
    "        if len(ar[0])==0:\n",
    "            print(f\"Site {i} has no data for year {j+2007}\")\n",
    "            count+=1\n",
    "print('Total number of missing data cases: ', count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Site 0000 has 7 problems\n",
      "Site 0001 has 1 problems\n",
      "Site 0002 has 1 problems\n",
      "Site 0003 has 7 problems\n",
      "Site 0004 has 3 problems\n",
      "Site 0005 has 1 problems\n",
      "Site 0006 has 3 problems\n",
      "Site 0007 has 2 problems\n",
      "Site 0008 has 2 problems\n",
      "Site 0009 has 1 problems\n",
      "Site 0010 has 1 problems\n",
      "Site 0011 has 3 problems\n",
      "Site 0012 has 1 problems\n",
      "Site 0013 has 1 problems\n",
      "Site 0014 has 1 problems\n",
      "Site 0015 has 2 problems\n",
      "Site 0016 has 1 problems\n",
      "Site 0017 has 4 problems\n",
      "Site 0018 has 2 problems\n",
      "Site 0019 has 12 problems\n",
      "Site 0020 has 4 problems\n",
      "Site 0021 has 2 problems\n",
      "Site 0022 has 1 problems\n",
      "Site 0023 has 1 problems\n",
      "Site 0024 has 3 problems\n",
      "Site 0025 has 1 problems\n",
      "Site 0026 has 3 problems\n",
      "Site 0027 has 3 problems\n",
      "Site 0029 has 1 problems\n",
      "Site 0030 has 2 problems\n",
      "Site 0031 has 5 problems\n",
      "Site 0032 has 2 problems\n",
      "Site 0033 has 1 problems\n",
      "Site 0034 has 9 problems\n",
      "Site 0035 has 11 problems\n",
      "Site 0036 has 1 problems\n",
      "Site 0037 has 1 problems\n",
      "Site 0038 has 3 problems\n",
      "Site 0039 has 10 problems\n",
      "Site 0040 has 1 problems\n",
      "Site 0041 has 7 problems\n",
      "Site 0042 has 2 problems\n",
      "Site 0043 has 4 problems\n",
      "Site 0044 has 2 problems\n",
      "Site 0045 has 10 problems\n",
      "Site 0046 has 4 problems\n",
      "Site 0047 has 6 problems\n",
      "Site 0049 has 13 problems\n",
      "Site 0050 has 2 problems\n",
      "Site 0051 has 4 problems\n",
      "Site 0052 has 14 problems\n",
      "Site 0053 has 1 problems\n",
      "Site 0054 has 3 problems\n",
      "Site 0055 has 3 problems\n",
      "Site 0056 has 2 problems\n",
      "Site 0057 has 9 problems\n",
      "Site 0058 has 1 problems\n",
      "Site 0059 has 1 problems\n",
      "Site 0060 has 1 problems\n",
      "Site 0061 has 2 problems\n",
      "Site 0062 has 1 problems\n",
      "Site 0063 has 9 problems\n",
      "Site 0064 has 3 problems\n",
      "Site 0067 has 3 problems\n",
      "Site 0068 has 9 problems\n",
      "Site 0069 has 9 problems\n",
      "Site 0070 has 3 problems\n",
      "Site 0071 has 10 problems\n",
      "Site 0072 has 2 problems\n",
      "Site 0073 has 4 problems\n",
      "Site 0075 has 4 problems\n",
      "Site 0076 has 8 problems\n",
      "Site 0078 has 1 problems\n",
      "Site 0079 has 1 problems\n",
      "Site 0080 has 2 problems\n",
      "Site 0081 has 2 problems\n",
      "Site 0082 has 8 problems\n",
      "Site 0083 has 3 problems\n",
      "Site 0084 has 5 problems\n",
      "Site 0085 has 3 problems\n",
      "Site 0087 has 3 problems\n",
      "Site 0088 has 7 problems\n",
      "Site 0089 has 1 problems\n",
      "Site 0090 has 2 problems\n",
      "Site 0091 has 5 problems\n",
      "Site 0092 has 3 problems\n",
      "Site 0094 has 9 problems\n",
      "Site 0095 has 10 problems\n",
      "Site 0096 has 2 problems\n",
      "Site 0097 has 1 problems\n",
      "Site 0098 has 12 problems\n",
      "Site 0099 has 1 problems\n",
      "Site 0100 has 2 problems\n",
      "Site 0101 has 1 problems\n",
      "Site 0102 has 2 problems\n",
      "Site 0103 has 2 problems\n",
      "Site 0104 has 1 problems\n",
      "Site 0105 has 13 problems\n",
      "Site 0106 has 10 problems\n",
      "Site 0107 has 3 problems\n",
      "Site 0108 has 5 problems\n",
      "Site 0109 has 1 problems\n",
      "Site 0110 has 9 problems\n",
      "Site 0111 has 2 problems\n",
      "Site 0112 has 6 problems\n",
      "Site 0114 has 10 problems\n",
      "Site 0115 has 12 problems\n",
      "Site 0116 has 9 problems\n",
      "Site 0118 has 1 problems\n",
      "Site 0119 has 1 problems\n",
      "Site 0121 has 1 problems\n",
      "Site 0122 has 2 problems\n",
      "Site 0123 has 2 problems\n",
      "Site 0125 has 3 problems\n",
      "Site 0126 has 1 problems\n",
      "Site 0127 has 2 problems\n",
      "Site 0128 has 10 problems\n",
      "Site 0129 has 11 problems\n",
      "Site 0130 has 1 problems\n",
      "Site 0131 has 1 problems\n",
      "Site 0132 has 5 problems\n",
      "Site 0133 has 2 problems\n",
      "Site 0134 has 2 problems\n",
      "Site 0135 has 7 problems\n",
      "Site 0136 has 1 problems\n",
      "Site 0137 has 10 problems\n",
      "Site 0138 has 9 problems\n",
      "Site 0139 has 2 problems\n",
      "Site 0140 has 1 problems\n",
      "Site 0142 has 1 problems\n",
      "Site 0143 has 2 problems\n",
      "Site 0145 has 3 problems\n",
      "Site 0146 has 8 problems\n",
      "Site 0147 has 2 problems\n",
      "Site 0148 has 1 problems\n",
      "Site 0150 has 3 problems\n",
      "Site 0151 has 1 problems\n",
      "Site 0152 has 1 problems\n",
      "Site 0153 has 7 problems\n",
      "Site 0154 has 4 problems\n",
      "Site 0155 has 2 problems\n",
      "Site 0157 has 3 problems\n",
      "Site 0158 has 1 problems\n",
      "Site 0159 has 11 problems\n",
      "Site 0160 has 5 problems\n",
      "Site 0161 has 1 problems\n",
      "Site 0162 has 2 problems\n",
      "Site 0163 has 1 problems\n",
      "Site 0164 has 3 problems\n",
      "Site 0165 has 2 problems\n",
      "Site 0166 has 5 problems\n",
      "Site 0167 has 9 problems\n",
      "Site 0170 has 4 problems\n",
      "Site 0171 has 2 problems\n",
      "Site 0172 has 1 problems\n",
      "Site 0173 has 11 problems\n",
      "Site 0174 has 1 problems\n",
      "Site 0176 has 2 problems\n",
      "Site 0177 has 3 problems\n",
      "Site 0178 has 1 problems\n",
      "Site 0179 has 2 problems\n",
      "Site 0180 has 5 problems\n",
      "Site 0181 has 1 problems\n",
      "Site 0182 has 2 problems\n",
      "Site 0185 has 4 problems\n",
      "Site 0186 has 11 problems\n",
      "Site 0188 has 2 problems\n",
      "Site 0189 has 10 problems\n",
      "Site 0190 has 1 problems\n",
      "Site 0191 has 1 problems\n",
      "Site 0192 has 4 problems\n",
      "Site 0193 has 3 problems\n",
      "Site 0194 has 1 problems\n",
      "Site 0195 has 1 problems\n",
      "Site 0196 has 4 problems\n",
      "Site 0197 has 1 problems\n",
      "Site 0199 has 11 problems\n"
     ]
    }
   ],
   "source": [
    "siteswithproblems=[]\n",
    "for i in np.unique(np.hstack(Warn)):\n",
    "    siteswithproblems.append(i[:4])\n",
    "temp=np.unique(siteswithproblems, return_counts=True)\n",
    "st,ct=temp[0],temp[1]\n",
    "for i,j in zip(st,ct):\n",
    "    print(f\"Site {i} has {j} problems\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current projection is: EPSG 5070\n",
      " Reporjecting to WGS84 (EPSG 4326)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Vito\\miniconda3\\envs\\gee\\Lib\\site-packages\\pyproj\\crs\\crs.py:141: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  in_crs_string = _prepare_from_proj_string(in_crs_string)\n",
      "c:\\Users\\Vito\\miniconda3\\envs\\gee\\Lib\\site-packages\\pyproj\\crs\\crs.py:141: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  in_crs_string = _prepare_from_proj_string(in_crs_string)\n",
      "C:\\Users\\Vito\\AppData\\Local\\Temp\\ipykernel_9712\\1492638990.py:11: FutureWarning: This function is deprecated. See: https://pyproj4.github.io/pyproj/stable/gotchas.html#upgrading-to-pyproj-2-from-pyproj-1\n",
      "  lon, lat = transform(source_projection, target_projection, x, y)\n"
     ]
    }
   ],
   "source": [
    "#getting the center coordinate of all sites\n",
    "epsg_code = gdf.crs.to_epsg()\n",
    "print(f\"The current projection is: EPSG {epsg_code}\\n Reporjecting to WGS84 (EPSG 4326)\")\n",
    "source_projection = Proj(init=f'epsg:{epsg_code}')\n",
    "target_projection = Proj(init='epsg:4326')  # WGS84\n",
    "\n",
    "#epsgcenterlist=[]\n",
    "wgscenterlist=[]\n",
    "for geom in geometry:\n",
    "    x, y = geom.centroid.x, geom.centroid.y\n",
    "    lon, lat = transform(source_projection, target_projection, x, y)\n",
    "    #epsgcenterlist.append([x, y])\n",
    "    wgscenterlist.append([lon, lat])\n",
    "np.save('Data/'+Case+'/masklayers/wgscenterlist.npy', wgscenterlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading NorthDakota NDVI: 100%|██████████| 30/30 [12:19:43<00:00, 1479.46s/site]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed sites: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#MODIS NDVI download\n",
    "NDVI_failed_sites = []\n",
    "step=1\n",
    "for i in tqdm(np.arange(0,num_sites,1),'Downloading '+Case+' NDVI', unit='site'):\n",
    "    try:\n",
    "        templist=[]\n",
    "        for yt in np.arange(2006,2023,step):\n",
    "            templist.append(getsiteNDVI(roiall, NDVICollection, i, f'{yt}-01-01', f'{yt+step}-01-01', 30))\n",
    "        St=pd.concat(templist, axis=0, join='inner')\n",
    "        St.to_hdf(sites_pth+f'/Site{i:03}_NBARint.h5', key='df', mode='w')\n",
    "    except Exception as e:\n",
    "        print(f\"Error: download failed for site {i} at year {yt}\")\n",
    "        NDVI_failed_sites.append(i)  \n",
    "print(f\"Failed sites: {NDVI_failed_sites}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NDVI download\n",
    "failed_sites = []\n",
    "for i in [12,26,27,73,89]:\n",
    "    templist=[]\n",
    "    for yt in np.arange(2006,2023,1):\n",
    "        try:\n",
    "            templist.append(getsiteNDVI(roiall, NDVICollection, i, f'{yt}-01-01', f'{yt}-04-01', 30))\n",
    "            templist.append(getsiteNDVI(roiall, NDVICollection, i, f'{yt}-04-01', f'{yt}-07-01', 30))\n",
    "            templist.append(getsiteNDVI(roiall, NDVICollection, i, f'{yt}-07-01', f'{yt}-10-01', 30))\n",
    "            templist.append(getsiteNDVI(roiall, NDVICollection, i, f'{yt}-10-01', f'{yt+1}-01-01', 30))\n",
    "        except Exception as e:\n",
    "            print(f\"Error: download failed for site {i} year {yt}\")\n",
    "            failed_sites.append(f'{i}_{yt}')\n",
    "    St=pd.concat(templist, axis=0, join='inner')\n",
    "    St.to_hdf(sites_pth+f'/Site{i:03}_NBARint.h5', key='df', mode='w')\n",
    "print(f\"Failed sites: {failed_sites}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "problem for season download\n",
    "site 12\n",
    "26\n",
    "73\n",
    "79\n",
    "89\n",
    "works after spliting in 5 years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: download failed for site 12\n"
     ]
    }
   ],
   "source": [
    "#NDSI season download\n",
    "NDSI_failed_sites = []\n",
    "for i in tqdm(np.arange(0,num_sites,1),'Downloading '+Case+' NDSI', unit='site'):\n",
    "    try:\n",
    "        d1,snow1,lst1=Season(roiall, i, 2006, 2013)\n",
    "        d2,snow2,lst2=Season(roiall, i, 2013, 2023)\n",
    "        St1=pd.concat([d1,d2], axis=0, join='inner')\n",
    "        St1.to_hdf(season_pth+f'/Site{i:03}_season_day.h5', key='df', mode='w')\n",
    "        St2=pd.concat([snow1,snow2], axis=0, join='inner')\n",
    "        St2.to_hdf(season_pth+f'/Site{i:03}_snow_ts.h5', key='df', mode='w') \n",
    "        St3=pd.concat([lst1,lst2], axis=0, join='inner')\n",
    "        St3.to_hdf(season_pth+f'/Site{i:03}_lst_ts.h5', key='df', mode='w') \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: download failed for site {i}\")\n",
    "        NDSI_failed_sites.append(i)\n",
    "print(f\"Failed sites: {NDSI_failed_sites}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "26\n",
      "73\n",
      "79\n",
      "89\n",
      "Failed sites: []\n"
     ]
    }
   ],
   "source": [
    "#season download splited\n",
    "attempt=0\n",
    "while len(NDSI_failed_sites)>0:\n",
    "    print(f\"Retrying sites: {NDSI_failed_sites}\")\n",
    "    new_NDSI_failed_sites = []\n",
    "    for i in tqdm(NDSI_failed_sites,'Downloading '+Case+' NDSI', unit='site'):\n",
    "        try:\n",
    "            d1,snow1,lst1=Season(roiall, i, 2006, 2010)\n",
    "            d2,snow2,lst2=Season(roiall, i, 2010, 2015)\n",
    "            d3,snow3,lst3=Season(roiall, i, 2015, 2020)\n",
    "            d4,snow4,lst4=Season(roiall, i, 2020, 2023)\n",
    "            \n",
    "            St1=pd.concat([d1,d2,d3,d4], axis=0, join='inner')\n",
    "            St1.to_hdf(season_pth+f'/Site{i:03}_season_day.h5', key='df', mode='w')\n",
    "            St2=pd.concat([snow1,snow2,snow3,snow4], axis=0, join='inner')\n",
    "            St2.to_hdf(season_pth+f'/Site{i:03}_snow_ts.h5', key='df', mode='w') \n",
    "            St3=pd.concat([lst1,lst2,lst3,lst4], axis=0, join='inner')\n",
    "            St3.to_hdf(season_pth+f'/Site{i:03}_lst_ts.h5', key='df', mode='w') \n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error: download failed for site {i}\")\n",
    "            new_NDSI_failed_sites.append(i)\n",
    "    if NDSI_failed_sites == new_NDSI_failed_sites:\n",
    "        print(f\"Error: number of failed sites not reducing. May need further splitting\")\n",
    "        break\n",
    "    NDSI_failed_sites = new_NDSI_failed_sites\n",
    "    print(f\"Failed sites: {failed_sites}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: download failed for site 121\n",
      "Failed sites: [121]\n"
     ]
    }
   ],
   "source": [
    "#site cdl donwload\n",
    "cdl_failed_sites = []\n",
    "for i in tqdm(np.arange(0,num_sites,1),'Downloading '+Case+' CDL', unit='site'):\n",
    "    try:\n",
    "        temp=cdldf(cdl, i)\n",
    "        temp.to_hdf(cdl_pth+f'/Site{i:03}_cdl.h5', key='df', mode='w') \n",
    "    except Exception as e:\n",
    "        print(f\"Error: download failed for site {i}\")\n",
    "        cdl_failed_sites.append(i)\n",
    "print(f\"First run failed sites: {cdl_failed_sites}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attempt=0\n",
    "while len(cdl_failed_sites)>0:\n",
    "    new_cdl_failed_sites = []\n",
    "    for i in tqdm(cdl_failed_sites,'Downloading '+Case+' CDL', unit='site'):\n",
    "        try:\n",
    "            temp=cdldf(cdl, i)\n",
    "            temp.to_hdf(cdl_pth+f'/Site{i:03}_cdl.h5', key='df', mode='w') \n",
    "        except Exception as e:\n",
    "            print(f\"Error: download failed for site {i}\")\n",
    "            new_cdl_failed_sites.append(i)\n",
    "    if cdl_failed_sites == new_cdl_failed_sites:\n",
    "        print(f\"Error: number of failed sites not reducing. May need further splitting\")\n",
    "        break\n",
    "    cdl_failed_sites = new_cdl_failed_sites\n",
    "    print(f\"Failed sites attempt {attempt}: {cdl_failed_sites}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed sites: []\n"
     ]
    }
   ],
   "source": [
    "#sentinel 1 donwload\n",
    "failed_sites = []\n",
    "for i in tqdm(np.arange(0,num_sites,1),'Downloading '+Case+' Sentinel 1', unit='site'):\n",
    "    templist=[]\n",
    "    try:\n",
    "        df = s1data(2015, 2023, i, direction = 'Decending', orbit = None, dataset='LOG')\n",
    "        df.to_hdf(sentinel1_pth+f'/Site{i:03}_db.h5', key='df', mode='w') \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: download failed for site {i}\")\n",
    "        failed_sites.append(i)\n",
    "print(f\"Failed sites: {failed_sites}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gee",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
