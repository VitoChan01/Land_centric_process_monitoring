{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import geemap\n",
    "import ee\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from pyproj import Proj, transform\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: authentication needed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p>To authorize access needed by Earth Engine, open the following\n",
       "        URL in a web browser and follow the instructions:</p>\n",
       "        <p><a href=https://code.earthengine.google.com/client-auth?scopes=https%3A//www.googleapis.com/auth/earthengine%20https%3A//www.googleapis.com/auth/devstorage.full_control&request_id=mbSI3nwapjs_rwL87P4gUUS6_Nwdl6fnLgWt4-uSl3Q&tc=oF4KewAVVTyzev_x132SOV7nQWt2N6pXJXDVzfPRSLc&cc=Aj6G6w1Ym_lpfZ4YICc6c_gr5fUnND_Se38qVnvMN_k>https://code.earthengine.google.com/client-auth?scopes=https%3A//www.googleapis.com/auth/earthengine%20https%3A//www.googleapis.com/auth/devstorage.full_control&request_id=mbSI3nwapjs_rwL87P4gUUS6_Nwdl6fnLgWt4-uSl3Q&tc=oF4KewAVVTyzev_x132SOV7nQWt2N6pXJXDVzfPRSLc&cc=Aj6G6w1Ym_lpfZ4YICc6c_gr5fUnND_Se38qVnvMN_k</a></p>\n",
       "        <p>The authorization workflow will generate a code, which you should paste in the box below.</p>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully saved authorization token.\n",
      "Initialization successful\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    ee.Initialize()\n",
    "    print(\"Initialization successful\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: authentication needed\")\n",
    "    ee.Authenticate()\n",
    "    ee.Initialize()\n",
    "    print(\"Initialization successful\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TS function\n",
    "def dailyNBARmaskFunc(img):\n",
    "    qa = img.select('BRDF_Albedo_Band_Mandatory_Quality_Band1')\n",
    "    qa2 = img.select('BRDF_Albedo_Band_Mandatory_Quality_Band2')\n",
    "    Quality = bitwiseExtract(qa, 0)\n",
    "    Qualityb2 = bitwiseExtract(qa2, 0)\n",
    "    mask = Quality.eq(0)\\\n",
    "        .And(Qualityb2.eq(0))\\\n",
    "\n",
    "\n",
    "    maskedImage = img.updateMask(mask)\n",
    "\n",
    "    return maskedImage\n",
    "\n",
    "def dailyNBARNDVI(img):\n",
    "    ndvi = img.normalizedDifference(['Nadir_Reflectance_Band2', 'Nadir_Reflectance_Band1']).rename('NDVI').set('system:time_start', img.get('system:time_start'))\n",
    "    return img.addBands(ndvi)\n",
    "\n",
    "def bitwiseExtract(value, fromBit, toBit=None):\n",
    "    '''\n",
    "    https://gis.stackexchange.com/questions/349371/creating-cloud-free-images-out-of-a-mod09a1-modis-image-in-gee/349401#349401\n",
    "    '''\n",
    "    if toBit == None:\n",
    "        toBit = fromBit\n",
    "    maskSize = ee.Number(1).add(toBit).subtract(fromBit)\n",
    "    mask = ee.Number(1).leftShift(maskSize).subtract(1)\n",
    "    return value.rightShift(fromBit).bitwiseAnd(mask)\n",
    "\n",
    "def Getroi(img):\n",
    "    \n",
    "    maskedImage = img.clip(roi)\n",
    "    \n",
    "    return maskedImage\n",
    "def rescale(image):\n",
    "    date = image.get('system:time_start')\n",
    "    return image.multiply(scale_factor).set('system:time_start', date)\n",
    "\n",
    "def createTS(image):\n",
    "    date = image.get('system:time_start')\n",
    "    value = image.reduceRegion(reducer=ee.Reducer.mean(), geometry=roi).get(var)\n",
    "    std = image.reduceRegion(reducer=ee.Reducer.stdDev(), geometry=roi).get(var)\n",
    "    ft = ee.Feature(None, {'date': ee.Date(date).format('Y/M/d'), var: value, 'STD': std})\n",
    "    return ft\n",
    "\n",
    "def TS_to_pandas(TS):\n",
    "    dump = TS.getInfo()\n",
    "    fts = dump['features']\n",
    "    out_vals = np.empty((len(fts)))\n",
    "    out_dates = []\n",
    "    out_std = np.empty((len(fts)))\n",
    "    \n",
    "    for i, f in enumerate(fts):\n",
    "        props = f['properties']\n",
    "        date = props['date']\n",
    "        val = props[var]\n",
    "        std = props['STD']\n",
    "        out_vals[i] = val\n",
    "        out_std[i] = std\n",
    "        out_dates.append(pd.Timestamp(date))\n",
    "    \n",
    "    df = pd.DataFrame({'mean' : out_vals, 'std' : out_std}, index=out_dates)\n",
    "    return df\n",
    "\n",
    "\n",
    "#GEE interpolation cloud comp\n",
    "#credit https://spatialthoughts.com/2021/11/08/temporal-interpolation-gee/\n",
    "def interpolate(image):\n",
    "    image = ee.Image(image)\n",
    "    date = image.get('system:time_start')\n",
    "    beforeImages = ee.List(image.get('before'))\n",
    "    beforeMosaic = ee.ImageCollection.fromImages(beforeImages).mosaic()\n",
    "    afterImages = ee.List(image.get('after'))\n",
    "    afterMosaic = ee.ImageCollection.fromImages(afterImages).mosaic()\n",
    "    \n",
    "    t1 = beforeMosaic.select('timestamp').rename('t1')\n",
    "    t2 = afterMosaic.select('timestamp').rename('t2')\n",
    "    t = image.metadata('system:time_start').rename('t')\n",
    "    tImage = ee.Image.cat([t1, t2, t])\n",
    "    timeRatio = tImage.expression('(t - t1) / (t2 - t1)', {'t': tImage.select('t'),'t1': tImage.select('t1'),'t2': tImage.select('t2')})\n",
    "\n",
    "    interpolated = beforeMosaic.add((afterMosaic.subtract(beforeMosaic).multiply(timeRatio)))\n",
    "    result = image.unmask(interpolated)\n",
    "    return result.copyProperties(image, ['system:time_start'])\n",
    "\n",
    "def timeImage(image):\n",
    "    tI = image.metadata('system:time_start').rename('timestamp')\n",
    "    timeImageMasked = tI.updateMask(image.mask().select(14))\n",
    "    return image.select('NDVI').addBands(timeImageMasked)\n",
    "\n",
    "def getsiteNDVI(roiall, NDVICollection, siteID, y_start, y_end, n):\n",
    "    coords = roiall.getInfo()['features'][siteID]['geometry']['coordinates']\n",
    "    global roi\n",
    "    roi = ee.Geometry.MultiPolygon(coords)\n",
    "\n",
    "    NDVI_flt = NDVICollection.filter(ee.Filter.date(y_start, y_end))\n",
    "    NDVImasked = NDVI_flt.map(dailyNBARmaskFunc)\n",
    "    NDVI_roimasked = NDVImasked.map(Getroi)\n",
    "\n",
    "    NDVI_rescale = NDVI_roimasked.map(rescale)\n",
    "    NDVI_rescale = NDVI_rescale.map(dailyNBARNDVI)\n",
    "    #pixel based interpolation\n",
    "    days = n\n",
    "\n",
    "    millis = ee.Number(days).multiply(1000*60*60*24)\n",
    "\n",
    "    NDVI_rescale_t = NDVI_rescale.map(timeImage)\n",
    "\n",
    "    maxDiffFilter = ee.Filter.maxDifference(**{'difference': millis,'leftField': 'system:time_start','rightField': 'system:time_start'})\n",
    "\n",
    "    lessEqFilter = ee.Filter.lessThanOrEquals(**{'leftField': 'system:time_start','rightField': 'system:time_start'})\n",
    "\n",
    "    greaterEqFilter = ee.Filter.greaterThanOrEquals(**{'leftField': 'system:time_start','rightField': 'system:time_start'})\n",
    "\n",
    "    filter1 = ee.Filter.And(maxDiffFilter, lessEqFilter)\n",
    "\n",
    "    join1 = ee.Join.saveAll(**{'matchesKey': 'after','ordering': 'system:time_start','ascending': False})\n",
    "\n",
    "    join1Result = join1.apply(**{'primary': NDVI_rescale_t,'secondary': NDVI_rescale_t,'condition': filter1})\n",
    "\n",
    "    filter2 = ee.Filter.And(maxDiffFilter, greaterEqFilter)\n",
    "\n",
    "    join2 = ee.Join.saveAll(**{'matchesKey': 'before','ordering': 'system:time_start','ascending': True})\n",
    "\n",
    "    join2Result = join2.apply(**{'primary':join1Result, 'secondary': join1Result, 'condition': filter2})\n",
    "\n",
    "    # Map the interpolation function over the image collection\n",
    "    interpolated_collection = ee.ImageCollection(join2Result.map(interpolate))\n",
    "    #interpolated_collection_ndvi =interpolated_collection.select(['NDVI']).map(lambda img:img.multiply(1).copyProperties(img, **{'properties':['system:time_start', 'system:index']}))\n",
    "    TS = interpolated_collection.map(createTS)\n",
    "    NDVI_ts_int = TS_to_pandas(TS)\n",
    "    return NDVI_ts_int\n",
    "\n",
    "#Season functions\n",
    "\n",
    "def snowdf(roiall,snow, siteID):\n",
    "    coords = roiall.getInfo()['features'][siteID]['geometry']['coordinates']\n",
    "    global roi, var\n",
    "    roi = ee.Geometry.MultiPolygon(coords)\n",
    "    var = 'NDSI_Snow_Cover'\n",
    "\n",
    "    snowcover = snow.select('NDSI_Snow_Cover').sort('system:time_start').filterBounds(roi).map(Getroi)\n",
    "    ts=snowcover.map(createTS)\n",
    "    df=TS_to_pandas(ts)\n",
    "    return df\n",
    "\n",
    "def snowmaskFunc(img):\n",
    "    qa = img.select('NDSI_Snow_Cover_Basic_QA')\n",
    "    Quality = bitwiseExtract(qa, 0, 15) \n",
    "    mask = Quality.eq(0).Or(Quality.eq(1))\n",
    "\n",
    "\n",
    "    maskedImage = img.updateMask(mask)\n",
    "\n",
    "    return maskedImage\n",
    "\n",
    "\n",
    "def LSTdf(roiall, LST, siteID):\n",
    "    coords = roiall.getInfo()['features'][siteID]['geometry']['coordinates']\n",
    "    global roi, var\n",
    "    roi = ee.Geometry.MultiPolygon(coords)\n",
    "    var = 'LST_Day_1km'\n",
    "    \n",
    "    LSTC = LST.select(var).sort('system:time_start').filterBounds(roi).map(Getroi)\n",
    "    ts=LSTC.map(createTS)\n",
    "    df=TS_to_pandas(ts)\n",
    "    return df\n",
    "\n",
    "def LSTmaskFunc(img):\n",
    "    qa = img.select('QC_Day')\n",
    "    Quality = bitwiseExtract(qa, 0, 1)\n",
    "    Quality2 = bitwiseExtract(qa, 2, 3) \n",
    "    mask = Quality.eq(0).Or(Quality2.eq(0))\n",
    "\n",
    "    maskedImage = img.updateMask(mask)\n",
    "\n",
    "    return maskedImage\n",
    "\n",
    "def seasonrangeCal(snowts, LSTts, yt):\n",
    "    lastsnow = np.array((snowts.loc[yt]['mean'][:120]>50)).nonzero()[0]\n",
    "    startsnow = np.array((snowts.loc[yt]['mean'][-120:]>50)).nonzero()[0]\n",
    "    spring = np.array((LSTts.loc[yt]['mean'][:120]*0.02-273.15>0)).nonzero()[0]\n",
    "    winter = np.array((LSTts.loc[yt]['mean'][-120:]*0.02-273.15<0)).nonzero()[0]\n",
    "\n",
    "    if len(lastsnow)!=0:\n",
    "        start_season = snowts.loc[yt].index[lastsnow[-1]]\n",
    "    elif len(spring)!=0:\n",
    "        start_season = snowts.loc[yt].index[spring[0]]\n",
    "    else:\n",
    "        start_season = snowts.loc[yt].index[0]\n",
    "    if len(startsnow)!=0:\n",
    "        end_season = snowts.loc[yt].index[-120:][startsnow[0]]\n",
    "    elif len(winter)!=0:\n",
    "        end_season = snowts.loc[yt].index[-120:][winter[0]]\n",
    "    else: \n",
    "        end_season = snowts.loc[yt].index[-1]\n",
    "    return start_season, end_season\n",
    "\n",
    "def Season(roiall, siteID, y_start, y_end):\n",
    "    startL=[]\n",
    "    endL=[]\n",
    "    snow=ee.ImageCollection('MODIS/061/MOD10A1').filter(ee.Filter.date(f'{y_start}-01-01', f'{y_end}-01-01'))\n",
    "    snow=snow.map(snowmaskFunc)\n",
    "    snowts=snowdf(roiall, snow, siteID)\n",
    "\n",
    "    LST=ee.ImageCollection('MODIS/061/MOD11A1').filter(ee.Filter.date(f'{y_start}-01-01', f'{y_end}-01-01'))\n",
    "    LST=LST.map(LSTmaskFunc)\n",
    "    LSTts=LSTdf(roiall, LST, siteID)\n",
    "    Years = np.arange(y_start, y_end, 1)\n",
    "    for y in Years:\n",
    "        yt=f'{y}'\n",
    "        st,ed=seasonrangeCal(snowts, LSTts, yt)\n",
    "        #st,ed=seasonrangeCal(snowts,0 , yt)\n",
    "        startL.append(st)\n",
    "        endL.append(ed)\n",
    "    df=pd.DataFrame(data={'start_season' : startL, 'end_season' : endL}, index=Years)\n",
    "    return df, snowts, LSTts\n",
    "\n",
    "#cdl function\n",
    "def createTScdl(image):\n",
    "    date = image.get('system:time_start')\n",
    "    value = image.reduceRegion(reducer=ee.Reducer.median(), geometry=roi).get('cropland')\n",
    "    ft = ee.Feature(None, {'date': ee.Date(date).format('Y/M/d'), 'mean': value})\n",
    "    return ft\n",
    "\n",
    "def TS_to_pandascdl(TS):\n",
    "    dump = TS.getInfo()\n",
    "    fts = dump['features']\n",
    "    out_vals = np.empty((len(fts)))\n",
    "    out_dates = []\n",
    "    \n",
    "    for i, f in enumerate(fts):\n",
    "        props = f['properties']\n",
    "        date = props['date']\n",
    "        if len(f['properties'])==1:\n",
    "            val = 0\n",
    "        else:\n",
    "            val = props['mean']\n",
    "        out_vals[i] = val\n",
    "        out_dates.append(pd.Timestamp(date))\n",
    "    \n",
    "    df = pd.DataFrame({'crop' : out_vals}, index=out_dates)\n",
    "    return df\n",
    "\n",
    "def cdldf(cdl, siteID):\n",
    "    coords = roiall.getInfo()['features'][siteID]['geometry']['coordinates']\n",
    "    global roi\n",
    "    roi = ee.Geometry.MultiPolygon(coords)\n",
    "    \n",
    "    cropLandcover = cdl.select('cropland').sort('system:time_start').filterBounds(roi).map(Getroi)\n",
    "    ts=cropLandcover.map(createTScdl)\n",
    "    df=TS_to_pandascdl(ts)\n",
    "    df.index=df.index.year\n",
    "    return df\n",
    "\n",
    "#sentinel 1 function\n",
    "def createTSS1(image):\n",
    "    date = image.get('system:time_start')\n",
    "    orb = image.get('relativeOrbitNumber_start')\n",
    "    slice = image.get('sliceNumber')\n",
    "    VVvalue = image.reduceRegion(reducer=ee.Reducer.mean(), geometry=roi).get('VV')\n",
    "    VVmx = image.reduceRegion(reducer=ee.Reducer.max(), geometry=roi).get('VV')\n",
    "    VHvalue = image.reduceRegion(reducer=ee.Reducer.mean(), geometry=roi).get('VH')\n",
    "    VHmx = image.reduceRegion(reducer=ee.Reducer.max(), geometry=roi).get('VH')\n",
    "    ft = ee.Feature(None, {'date': ee.Date(date).format('Y/M/d'), 'VVmean': VVvalue, 'VVmax':VVmx, 'VHmean': VHvalue\n",
    "                           , 'VHmax':VHmx, 'orbit':orb, 'slice':slice})\n",
    "    return ft\n",
    "\n",
    "def TS_to_pandass1(TS):\n",
    "    dump = TS.getInfo()\n",
    "    fts = dump['features']\n",
    "    out_vals = np.empty((len(fts)))\n",
    "    out_vals2 = np.empty((len(fts)))\n",
    "    out_vals3 = np.empty((len(fts)))\n",
    "    out_vals4 = np.empty((len(fts)))\n",
    "    out_dates = []\n",
    "    out_orbits = []\n",
    "    out_slices = []\n",
    "    \n",
    "    for i, f in enumerate(fts):\n",
    "        props = f['properties']\n",
    "        date = props['date']\n",
    "        val = props['VVmean']\n",
    "        out_vals[i] = val\n",
    "        val = props['VVmax']\n",
    "        out_vals2[i] = val\n",
    "        val = props['VHmean']\n",
    "        out_vals3[i] = val\n",
    "        val = props['VHmax']\n",
    "        out_vals4[i] = val\n",
    "        out_dates.append(pd.Timestamp(date))\n",
    "        out_orbits.append(props['orbit'])\n",
    "        out_slices.append(props['slice'])\n",
    "    \n",
    "    df = pd.DataFrame({'VVmean' : out_vals, 'VVmax': out_vals2, 'VHmean' : out_vals3, 'VHmax': out_vals4\n",
    "                       , 'orbit':out_orbits, 'slice':out_slices}, index=out_dates)\n",
    "    return df\n",
    "\n",
    "def s1data(y_start, y_end, siteID, direction = 'Ascending', orbit = None, dataset='FLOAT'):\n",
    "    coords = roiall.getInfo()['features'][siteID]['geometry']['coordinates']\n",
    "    global roi, var\n",
    "    roi = ee.Geometry.MultiPolygon(coords)\n",
    "    if dataset == 'FLOAT':\n",
    "        S1 = ee.ImageCollection('COPERNICUS/S1_GRD_FLOAT') \n",
    "    elif dataset == 'LOG':\n",
    "        S1 = ee.ImageCollection('COPERNICUS/S1_GRD')#log-scaled (in dB)\n",
    "    S1 = S1.filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV'))\\\n",
    "        .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VH'))\\\n",
    "        .filter(ee.Filter.eq('instrumentMode', 'IW'))\\\n",
    "        .filterBounds(roi)\\\n",
    "        .filter(ee.Filter.date(f'{y_start}-01-01', f'{y_end}-01-01'))\n",
    "\n",
    "    if orbit:\n",
    "        S1 = S1.filter(ee.Filter.eq('relativeOrbitNumber_start', orbit))\n",
    "    if direction == 'Ascending':\n",
    "        data = S1.filter(ee.Filter.eq('orbitProperties_pass', 'ASCENDING'))\n",
    "    else:\n",
    "        data = S1.filter(ee.Filter.eq('orbitProperties_pass', 'DESCENDING'))\n",
    "    data = data.map(Getroi)\n",
    "    data = data.map(createTSS1)\n",
    "    df = TS_to_pandass1(data)\n",
    "    return df\n",
    "\n",
    "#CDL consistency function\n",
    "def dd(date):\n",
    "    return ee.Date(date).format('Y-MM-dd')\n",
    "def consist(ar):\n",
    "    if len(ar[1])!=0:\n",
    "        return np.max(ar[1])/np.sum(ar[1])\n",
    "    else:\n",
    "        return 0\n",
    "def warn_mark(i,n):\n",
    "    return f\"{i:04}_{n+2007}\"\n",
    "def warn_site_List(Lst):\n",
    "    return list(map(consist, Lst))\n",
    "def warn_site_years(Lst):\n",
    "    threshold=np.nonzero(np.array(Lst[1])<0.75)[0]\n",
    "    return list(map(lambda x: warn_mark(Lst[0], x), threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wisconsin loaded\n",
      "Number of sites: 109\n"
     ]
    }
   ],
   "source": [
    "#Case='Idaho'\n",
    "#Case='NorthDakota'\n",
    "#Case='Colorado'\n",
    "#Case='Washington'\n",
    "Case='Wisconsin'\n",
    "\n",
    "Start_year = 2008\n",
    "End_year = 2023\n",
    "\n",
    "sites_pth = 'Data/'+Case+'/sites'\n",
    "cdl_pth = 'Data/'+Case+'/cdl'\n",
    "season_pth = 'Data/'+Case+'/season'\n",
    "sentinel1_pth = 'Data/'+Case+'/sentinel1'\n",
    "\n",
    "if not os.path.exists(sites_pth):\n",
    "    os.makedirs(sites_pth)\n",
    "if not os.path.exists(cdl_pth):\n",
    "    os.makedirs(cdl_pth)\n",
    "if not os.path.exists(season_pth):\n",
    "    os.makedirs(season_pth)\n",
    "if not os.path.exists(sentinel1_pth):\n",
    "    os.makedirs(sentinel1_pth)\n",
    "\n",
    "#define variables\n",
    "roi_shp = 'Data/'+Case+'/masklayers/site_mask.shp'\n",
    "roiall = geemap.shp_to_ee(roi_shp)\n",
    "cdl=ee.ImageCollection('USDA/NASS/CDL').filter(ee.Filter.date('2008-01-01', '2023-01-01'))\n",
    "NDVICollection = ee.ImageCollection('MODIS/061/MCD43A4').filter(ee.Filter.date('2000-01-01', '2023-01-01'))\n",
    "var = 'NDVI'\n",
    "scale_factor = 0.0001\n",
    "\n",
    "gdf = gpd.read_file(roi_shp)\n",
    "geometry = gdf['geometry']\n",
    "num_sites=len(geometry)\n",
    "print(Case+' loaded')\n",
    "print(f\"Number of sites: {num_sites}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No missing files in Data/Wisconsin/sites\n",
      "No missing files in Data/Wisconsin/cdl\n",
      "No missing files in Data/Wisconsin/season\n",
      "Missing files in Data/Wisconsin/sentinel1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for set in [sites_pth, cdl_pth, season_pth, sentinel1_pth]:\n",
    "    file_names = os.listdir(set)\n",
    "    if len(file_names)<num_sites:\n",
    "        print(f\"Missing files in {set}\")\n",
    "        print(len(file_names))\n",
    "    else:\n",
    "        print(f\"No missing files in {set}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Wisconsin CDL Consistency: 100%|██████████| 109/109 [35:30<00:00, 19.55s/site]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of multicrop cases:  445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#checking crop on site\n",
    "AlldistL=[]\n",
    "for i in tqdm(np.arange(0,num_sites,1),'Downloading '+Case+' CDL Consistency', unit='site'):\n",
    "    coords = roiall.getInfo()['features'][i]['geometry']['coordinates']\n",
    "    roi = ee.Geometry.MultiPolygon(coords)\n",
    "\n",
    "    #create image collection of interested time frame and area and sort by date\n",
    "    im = ((cdl.filterBounds(roi)).map(Getroi)).sort('system:time_start')\n",
    "    #get date list\n",
    "    Acqt=np.sort(im.aggregate_array('system:time_start').map(dd).getInfo())\n",
    "    #convert to list for indexing\n",
    "    imL = im.toList(im.size())\n",
    "\n",
    "    distL=[]\n",
    "    for i in range(Acqt.size):\n",
    "        image=ee.Image(imL.get(i))\n",
    "        imar=ee.ImageCollection(image).getRegion(roi, scale=30).getInfo()\n",
    "        df = pd.DataFrame(imar[1:], columns=imar[0])\n",
    "        #if df['cropland'].notnull().any():\n",
    "        dist=np.array(np.unique(df['cropland'], return_counts=True))\n",
    "        distL.append(dist)\n",
    "    AlldistL.append(distL)\n",
    "site_consistencyL=list(map(warn_site_List, AlldistL))\n",
    "Warn=list(map(warn_site_years, enumerate(site_consistencyL)))\n",
    "\n",
    "count=0\n",
    "for i in Warn:\n",
    "    count+=len(i)\n",
    "print('Total number of multicrop cases: ', count)\n",
    "\n",
    "with open(cdl_pth+\"/CaseID_anom\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(Warn, fp)\n",
    "with open(cdl_pth+\"/ConsistencyPerc\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(site_consistencyL, fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of missing data cases:  0\n"
     ]
    }
   ],
   "source": [
    "#report multicrop\n",
    "count=0\n",
    "for i in range(len(AlldistL)):\n",
    "    lst=AlldistL[i]\n",
    "    for j in range(len(lst)):\n",
    "        ar=lst[j]\n",
    "        if len(ar[0])==0:\n",
    "            print(f\"Site {i} has no data for year {j+2007}\")\n",
    "            count+=1\n",
    "print('Total number of missing data cases: ', count)\n",
    "\n",
    "siteswithproblems=[]\n",
    "for i in np.unique(np.hstack(Warn)):\n",
    "    siteswithproblems.append(i[:4])\n",
    "temp=np.unique(siteswithproblems, return_counts=True)\n",
    "st,ct=temp[0],temp[1]\n",
    "for i,j in zip(st,ct):\n",
    "    print(f\"Site {i} has {j} problem(s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Vito\\miniconda3\\envs\\gee\\Lib\\site-packages\\pyproj\\crs\\crs.py:141: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  in_crs_string = _prepare_from_proj_string(in_crs_string)\n",
      "c:\\Users\\Vito\\miniconda3\\envs\\gee\\Lib\\site-packages\\pyproj\\crs\\crs.py:141: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  in_crs_string = _prepare_from_proj_string(in_crs_string)\n",
      "C:\\Users\\Vito\\AppData\\Local\\Temp\\ipykernel_36504\\1492638990.py:11: FutureWarning: This function is deprecated. See: https://pyproj4.github.io/pyproj/stable/gotchas.html#upgrading-to-pyproj-2-from-pyproj-1\n",
      "  lon, lat = transform(source_projection, target_projection, x, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current projection is: EPSG 5070\n",
      " Reporjecting to WGS84 (EPSG 4326)\n"
     ]
    }
   ],
   "source": [
    "#getting the center coordinate of all sites\n",
    "epsg_code = gdf.crs.to_epsg()\n",
    "print(f\"The current projection is: EPSG {epsg_code}\\n Reporjecting to WGS84 (EPSG 4326)\")\n",
    "source_projection = Proj(init=f'epsg:{epsg_code}')\n",
    "target_projection = Proj(init='epsg:4326')  # WGS84\n",
    "\n",
    "#epsgcenterlist=[]\n",
    "wgscenterlist=[]\n",
    "for geom in geometry:\n",
    "    x, y = geom.centroid.x, geom.centroid.y\n",
    "    lon, lat = transform(source_projection, target_projection, x, y)\n",
    "    #epsgcenterlist.append([x, y])\n",
    "    wgscenterlist.append([lon, lat])\n",
    "np.save('Data/'+Case+'/masklayers/wgscenterlist.npy', wgscenterlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Wisconsin NDVI: 100%|██████████| 109/109 [38:33:48<00:00, 1273.66s/site]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First attempt, 1 site(s) failed:\n",
      "[90]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#MODIS NDVI download\n",
    "NDVI_failed_sites = []\n",
    "step=1\n",
    "var = 'NDVI'\n",
    "scale_factor = 0.0001\n",
    "for i in tqdm(np.arange(0,num_sites,1),'Downloading '+Case+' NDVI', unit='site'):\n",
    "    try:\n",
    "        templist=[]\n",
    "        for yt in np.arange(2006,2023,step):\n",
    "            templist.append(getsiteNDVI(roiall, NDVICollection, i, f'{yt}-01-01', f'{yt+step}-01-01', 30))\n",
    "        St=pd.concat(templist, axis=0, join='inner')\n",
    "        St.to_hdf(sites_pth+f'/Site{i:03}_NBARint.h5', key='df', mode='w')\n",
    "    except Exception as e:\n",
    "        #print(f\"Error: download failed for site {i} at year {yt}\")\n",
    "        NDVI_failed_sites.append(i)  \n",
    "print(f\"First attempt, {len(NDVI_failed_sites)} site(s) failed:\\n{NDVI_failed_sites}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrying sites: [90]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Wisconsin NVSI: 100%|██████████| 1/1 [15:32<00:00, 932.57s/site]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed sites attempt 1, 0 site(s) remains:\n",
      "[]\n",
      "Reducing step size to 5 months\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#retry failed cases\n",
    "attempt=0\n",
    "step=6\n",
    "while len(NDVI_failed_sites)>0:\n",
    "    if attempt>0:\n",
    "        print(f\"Failed sites attempt {attempt}, {len(NDVI_failed_sites)} site(s) remains:\\n{NDVI_failed_sites}\\nReducing step size to {step} months\")\n",
    "    print(f\"Retrying sites: {NDVI_failed_sites}\")\n",
    "    new_NDVI_failed_sites = []\n",
    "\n",
    "    for i in tqdm(NDVI_failed_sites,'Downloading '+Case+' NVSI', unit='site'):\n",
    "        try:\n",
    "            templist=[]\n",
    "            for yt in np.arange(2006,2023,1):\n",
    "                for month in np.arange(1,12-step,step):\n",
    "                    templist.append(getsiteNDVI(roiall, NDVICollection, i, f'{yt}-{month:02}-01', f'{yt}-{month+step:02}-01', 30))\n",
    "                templist.append(getsiteNDVI(roiall, NDVICollection, i, f'{yt}-{month+step:02}-01', f'{yt+1}-01-01', 30))\n",
    "            St=pd.concat(templist, axis=0, join='inner')\n",
    "            St.to_hdf(sites_pth+f'/Site{i:03}_NBARint.h5', key='df', mode='w')\n",
    "        except Exception as e:\n",
    "            #print(f\"Error: download failed for site {i} year {yt}\")\n",
    "            new_NDVI_failed_sites.append(i)\n",
    "    if ((step==1) and (NDVI_failed_sites == new_NDVI_failed_sites)):\n",
    "        print(f\"Error: Minimum splitting reached, {len(new_NDVI_failed_sites)} site(s) remaining\")\n",
    "        break\n",
    "    NDVI_failed_sites = new_NDVI_failed_sites\n",
    "    attempt+=1\n",
    "    step-=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Wisconsin NDSI:  86%|████████▌ | 94/109 [4:29:39<47:03, 188.22s/site]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: download failed for site 93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Wisconsin NDSI:  87%|████████▋ | 95/109 [4:31:06<36:48, 157.74s/site]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: download failed for site 94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Wisconsin NDSI:  88%|████████▊ | 96/109 [4:32:31<29:24, 135.76s/site]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: download failed for site 95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Wisconsin NDSI:  89%|████████▉ | 97/109 [4:33:28<22:26, 112.25s/site]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: download failed for site 96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Wisconsin NDSI: 100%|██████████| 109/109 [5:05:04<00:00, 167.93s/site]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed sites: [93, 94, 95, 96]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#NDSI season download\n",
    "NDSI_failed_sites = []\n",
    "for i in tqdm(np.arange(0,num_sites,1),'Downloading '+Case+' NDSI', unit='site'):\n",
    "    try:\n",
    "        d1,snow1,lst1=Season(roiall, i, 2006, 2013)\n",
    "        d2,snow2,lst2=Season(roiall, i, 2013, 2023)\n",
    "        St1=pd.concat([d1,d2], axis=0, join='inner')\n",
    "        St1.to_hdf(season_pth+f'/Site{i:03}_season_day.h5', key='df', mode='w')\n",
    "        St2=pd.concat([snow1,snow2], axis=0, join='inner')\n",
    "        St2.to_hdf(season_pth+f'/Site{i:03}_snow_ts.h5', key='df', mode='w') \n",
    "        St3=pd.concat([lst1,lst2], axis=0, join='inner')\n",
    "        St3.to_hdf(season_pth+f'/Site{i:03}_lst_ts.h5', key='df', mode='w') \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: download failed for site {i}\")\n",
    "        NDSI_failed_sites.append(i)\n",
    "print(f\"First attempt, {len(NDVI_failed_sites)} site(s) failed:\\n{NDSI_failed_sites}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#season download splited for failed cases\n",
    "attempt=0\n",
    "step=4\n",
    "while len(NDSI_failed_sites)>0:\n",
    "    print(f\"Retrying sites: {NDSI_failed_sites}\")\n",
    "    new_NDSI_failed_sites = []\n",
    "    for i in tqdm(NDSI_failed_sites,'Downloading '+Case+' NDSI', unit='site'):\n",
    "        try:\n",
    "            d_lst=[]\n",
    "            snow_lst=[]\n",
    "            lst_lst=[]\n",
    "            for y in np.arange(2006,2023,step):\n",
    "                d,snow,lst=Season(roiall, i, y, y+step)\n",
    "                d_lst.append(d)\n",
    "                snow_lst.append(snow)\n",
    "                lst_lst.append(lst)\n",
    "#            d1,snow1,lst1=Season(roiall, i, 2006, 2010)\n",
    "#            d2,snow2,lst2=Season(roiall, i, 2010, 2015)\n",
    "#            d3,snow3,lst3=Season(roiall, i, 2015, 2020)\n",
    "#            d4,snow4,lst4=Season(roiall, i, 2020, 2023)\n",
    "            \n",
    "            #St1=pd.concat([d1,d2,d3,d4], axis=0, join='inner')\n",
    "            St1=pd.concat(d_lst, axis=0, join='inner')\n",
    "            St1.to_hdf(season_pth+f'/Site{i:03}_season_day.h5', key='df', mode='w')\n",
    "            #St2=pd.concat([snow1,snow2,snow3,snow4], axis=0, join='inner')\n",
    "            St2=pd.concat(snow_lst, axis=0, join='inner')\n",
    "            St2.to_hdf(season_pth+f'/Site{i:03}_snow_ts.h5', key='df', mode='w') \n",
    "            #St3=pd.concat([lst1,lst2,lst3,lst4], axis=0, join='inner')\n",
    "            St3=pd.concat(lst_lst, axis=0, join='inner')\n",
    "            St3.to_hdf(season_pth+f'/Site{i:03}_lst_ts.h5', key='df', mode='w') \n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error: download failed for site {i}\")\n",
    "            new_NDSI_failed_sites.append(i)\n",
    "    if ((step==1) and (NDSI_failed_sites == new_NDSI_failed_sites)):\n",
    "        print(f\"Error: Minimum splitting reached, {len(new_NDSI_failed_sites)} site(s) remaining\")\n",
    "        break\n",
    "    NDSI_failed_sites = new_NDSI_failed_sites\n",
    "    attempt+=1\n",
    "    step-=1\n",
    "    print(f\"Failed sites attempt {attempt}, {len(NDSI_failed_sites)} site(s) remains:\\n{NDSI_failed_sites}\\nReducing step size to {step} months\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Wisconsin CDL: 100%|██████████| 109/109 [09:59<00:00,  5.50s/site]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First attempt failed sites: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#site cdl donwload\n",
    "cdl_failed_sites = []\n",
    "for i in tqdm(np.arange(0,num_sites,1),'Downloading '+Case+' CDL', unit='site'):\n",
    "    try:\n",
    "        temp=cdldf(cdl, i)\n",
    "        temp.to_hdf(cdl_pth+f'/Site{i:03}_cdl.h5', key='df', mode='w') \n",
    "    except Exception as e:\n",
    "        print(f\"Error: download failed for site {i}\")\n",
    "        cdl_failed_sites.append(i)\n",
    "print(f\"First attempt, {len(cdl_failed_sites)} site(s) failed:\\n{cdl_failed_sites}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "attempt=0\n",
    "while len(cdl_failed_sites)>0:\n",
    "    new_cdl_failed_sites = []\n",
    "    for i in tqdm(cdl_failed_sites,'Downloading '+Case+' CDL', unit='site'):\n",
    "        try:\n",
    "            temp=cdldf(cdl, i)\n",
    "            temp.to_hdf(cdl_pth+f'/Site{i:03}_cdl.h5', key='df', mode='w') \n",
    "        except Exception as e:\n",
    "            print(f\"Error: download failed for site {i}\")\n",
    "            new_cdl_failed_sites.append(i)\n",
    "    if cdl_failed_sites == new_cdl_failed_sites:\n",
    "        print(f\"Error: number of failed sites not reducing. May need further splitting\")\n",
    "        break\n",
    "    cdl_failed_sites = new_cdl_failed_sites\n",
    "    print(f\"Failed sites attempt {attempt}, {len(cdl_failed_sites)} site(s) remains:\\n{cdl_failed_sites}\")\n",
    "    attempt+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed sites: []\n"
     ]
    }
   ],
   "source": [
    "#sentinel 1 donwload\n",
    "S1_failed_sites = []\n",
    "for i in tqdm(np.arange(0,num_sites,1),'Downloading '+Case+' Sentinel 1', unit='site'):\n",
    "    templist=[]\n",
    "    try:\n",
    "        df = s1data(2015, 2023, i, direction = 'Decending', orbit = None, dataset='LOG')\n",
    "        df.to_hdf(sentinel1_pth+f'/Site{i:03}_db.h5', key='df', mode='w') \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: download failed for site {i}\")\n",
    "        S1_failed_sites.append(i)\n",
    "print(f\"Failed sites: {S1_failed_sites}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gee",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
