{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import geemap\n",
    "import ee\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from pyproj import Proj, transform\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization successful\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    ee.Initialize()\n",
    "    print(\"Initialization successful\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: authentication needed\")\n",
    "    ee.Authenticate()\n",
    "    ee.Initialize()\n",
    "    print(\"Initialization successful\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TS function\n",
    "def dailyNBARmaskFunc(img):\n",
    "    qa = img.select('BRDF_Albedo_Band_Mandatory_Quality_Band1')\n",
    "    qa2 = img.select('BRDF_Albedo_Band_Mandatory_Quality_Band2')\n",
    "    Quality = bitwiseExtract(qa, 0)\n",
    "    Qualityb2 = bitwiseExtract(qa2, 0)\n",
    "    mask = Quality.eq(0)\\\n",
    "        .And(Qualityb2.eq(0))\\\n",
    "\n",
    "\n",
    "    maskedImage = img.updateMask(mask)\n",
    "\n",
    "    return maskedImage\n",
    "\n",
    "def dailyNBARNDVI(img):\n",
    "    ndvi = img.normalizedDifference(['Nadir_Reflectance_Band2', 'Nadir_Reflectance_Band1']).rename('NDVI').set('system:time_start', img.get('system:time_start'))\n",
    "    return img.addBands(ndvi)\n",
    "\n",
    "def bitwiseExtract(value, fromBit, toBit=None):\n",
    "    '''\n",
    "    https://gis.stackexchange.com/questions/349371/creating-cloud-free-images-out-of-a-mod09a1-modis-image-in-gee/349401#349401\n",
    "    '''\n",
    "    if toBit == None:\n",
    "        toBit = fromBit\n",
    "    maskSize = ee.Number(1).add(toBit).subtract(fromBit)\n",
    "    mask = ee.Number(1).leftShift(maskSize).subtract(1)\n",
    "    return value.rightShift(fromBit).bitwiseAnd(mask)\n",
    "\n",
    "def Getroi(img):\n",
    "    \n",
    "    maskedImage = img.clip(roi)\n",
    "    \n",
    "    return maskedImage\n",
    "def rescale(image):\n",
    "    date = image.get('system:time_start')\n",
    "    return image.multiply(scale_factor).set('system:time_start', date)\n",
    "\n",
    "def createTS(image):\n",
    "    date = image.get('system:time_start')\n",
    "    value = image.reduceRegion(reducer=ee.Reducer.mean(), geometry=roi).get(var)\n",
    "    std = image.reduceRegion(reducer=ee.Reducer.stdDev(), geometry=roi).get(var)\n",
    "    ft = ee.Feature(None, {'date': ee.Date(date).format('Y/M/d'), var: value, 'STD': std})\n",
    "    return ft\n",
    "\n",
    "def TS_to_pandas(TS):\n",
    "    dump = TS.getInfo()\n",
    "    fts = dump['features']\n",
    "    out_vals = np.empty((len(fts)))\n",
    "    out_dates = []\n",
    "    out_std = np.empty((len(fts)))\n",
    "    \n",
    "    for i, f in enumerate(fts):\n",
    "        props = f['properties']\n",
    "        date = props['date']\n",
    "        val = props[var]\n",
    "        std = props['STD']\n",
    "        out_vals[i] = val\n",
    "        out_std[i] = std\n",
    "        out_dates.append(pd.Timestamp(date))\n",
    "    \n",
    "    df = pd.DataFrame({'mean' : out_vals, 'std' : out_std}, index=out_dates)\n",
    "    return df\n",
    "\n",
    "\n",
    "#GEE interpolation cloud comp\n",
    "#credit https://spatialthoughts.com/2021/11/08/temporal-interpolation-gee/\n",
    "def interpolate(image):\n",
    "    image = ee.Image(image)\n",
    "    date = image.get('system:time_start')\n",
    "    beforeImages = ee.List(image.get('before'))\n",
    "    beforeMosaic = ee.ImageCollection.fromImages(beforeImages).mosaic()\n",
    "    afterImages = ee.List(image.get('after'))\n",
    "    afterMosaic = ee.ImageCollection.fromImages(afterImages).mosaic()\n",
    "    \n",
    "    t1 = beforeMosaic.select('timestamp').rename('t1')\n",
    "    t2 = afterMosaic.select('timestamp').rename('t2')\n",
    "    t = image.metadata('system:time_start').rename('t')\n",
    "    tImage = ee.Image.cat([t1, t2, t])\n",
    "    timeRatio = tImage.expression('(t - t1) / (t2 - t1)', {'t': tImage.select('t'),'t1': tImage.select('t1'),'t2': tImage.select('t2')})\n",
    "\n",
    "    interpolated = beforeMosaic.add((afterMosaic.subtract(beforeMosaic).multiply(timeRatio)))\n",
    "    result = image.unmask(interpolated)\n",
    "    return result.copyProperties(image, ['system:time_start'])\n",
    "\n",
    "def timeImage(image):\n",
    "    tI = image.metadata('system:time_start').rename('timestamp')\n",
    "    timeImageMasked = tI.updateMask(image.mask().select(14))\n",
    "    return image.select('NDVI').addBands(timeImageMasked)\n",
    "\n",
    "def getsiteNDVI(roiall, NDVICollection, siteID, y_start, y_end, n):\n",
    "    coords = roiall.getInfo()['features'][siteID]['geometry']['coordinates']\n",
    "    global roi\n",
    "    roi = ee.Geometry.MultiPolygon(coords)\n",
    "\n",
    "    NDVI_flt = NDVICollection.filter(ee.Filter.date(y_start, y_end))\n",
    "    NDVImasked = NDVI_flt.map(dailyNBARmaskFunc)\n",
    "    NDVI_roimasked = NDVImasked.map(Getroi)\n",
    "\n",
    "    NDVI_rescale = NDVI_roimasked.map(rescale)\n",
    "    NDVI_rescale = NDVI_rescale.map(dailyNBARNDVI)\n",
    "    #pixel based interpolation\n",
    "    days = n\n",
    "\n",
    "    millis = ee.Number(days).multiply(1000*60*60*24)\n",
    "\n",
    "    NDVI_rescale_t = NDVI_rescale.map(timeImage)\n",
    "\n",
    "    maxDiffFilter = ee.Filter.maxDifference(**{'difference': millis,'leftField': 'system:time_start','rightField': 'system:time_start'})\n",
    "\n",
    "    lessEqFilter = ee.Filter.lessThanOrEquals(**{'leftField': 'system:time_start','rightField': 'system:time_start'})\n",
    "\n",
    "    greaterEqFilter = ee.Filter.greaterThanOrEquals(**{'leftField': 'system:time_start','rightField': 'system:time_start'})\n",
    "\n",
    "    filter1 = ee.Filter.And(maxDiffFilter, lessEqFilter)\n",
    "\n",
    "    join1 = ee.Join.saveAll(**{'matchesKey': 'after','ordering': 'system:time_start','ascending': False})\n",
    "\n",
    "    join1Result = join1.apply(**{'primary': NDVI_rescale_t,'secondary': NDVI_rescale_t,'condition': filter1})\n",
    "\n",
    "    filter2 = ee.Filter.And(maxDiffFilter, greaterEqFilter)\n",
    "\n",
    "    join2 = ee.Join.saveAll(**{'matchesKey': 'before','ordering': 'system:time_start','ascending': True})\n",
    "\n",
    "    join2Result = join2.apply(**{'primary':join1Result, 'secondary': join1Result, 'condition': filter2})\n",
    "\n",
    "    # Map the interpolation function over the image collection\n",
    "    interpolated_collection = ee.ImageCollection(join2Result.map(interpolate))\n",
    "    #interpolated_collection_ndvi =interpolated_collection.select(['NDVI']).map(lambda img:img.multiply(1).copyProperties(img, **{'properties':['system:time_start', 'system:index']}))\n",
    "    TS = interpolated_collection.map(createTS)\n",
    "    NDVI_ts_int = TS_to_pandas(TS)\n",
    "    return NDVI_ts_int\n",
    "\n",
    "#Season functions\n",
    "\n",
    "def snowdf(roiall,snow, siteID):\n",
    "    coords = roiall.getInfo()['features'][siteID]['geometry']['coordinates']\n",
    "    global roi, var\n",
    "    roi = ee.Geometry.MultiPolygon(coords)\n",
    "    var = 'NDSI_Snow_Cover'\n",
    "\n",
    "    snowcover = snow.select('NDSI_Snow_Cover').sort('system:time_start').filterBounds(roi).map(Getroi)\n",
    "    ts=snowcover.map(createTS)\n",
    "    df=TS_to_pandas(ts)\n",
    "    return df\n",
    "\n",
    "def snowmaskFunc(img):\n",
    "    qa = img.select('NDSI_Snow_Cover_Basic_QA')\n",
    "    Quality = bitwiseExtract(qa, 0, 15) \n",
    "    mask = Quality.eq(0).Or(Quality.eq(1))\n",
    "\n",
    "\n",
    "    maskedImage = img.updateMask(mask)\n",
    "\n",
    "    return maskedImage\n",
    "\n",
    "\n",
    "def LSTdf(roiall, LST, siteID):\n",
    "    coords = roiall.getInfo()['features'][siteID]['geometry']['coordinates']\n",
    "    global roi, var\n",
    "    roi = ee.Geometry.MultiPolygon(coords)\n",
    "    var = 'LST_Day_1km'\n",
    "    \n",
    "    LSTC = LST.select(var).sort('system:time_start').filterBounds(roi).map(Getroi)\n",
    "    ts=LSTC.map(createTS)\n",
    "    df=TS_to_pandas(ts)\n",
    "    return df\n",
    "\n",
    "def LSTmaskFunc(img):\n",
    "    qa = img.select('QC_Day')\n",
    "    Quality = bitwiseExtract(qa, 0, 1)\n",
    "    Quality2 = bitwiseExtract(qa, 2, 3) \n",
    "    mask = Quality.eq(0).Or(Quality2.eq(0))\n",
    "\n",
    "    maskedImage = img.updateMask(mask)\n",
    "\n",
    "    return maskedImage\n",
    "\n",
    "def seasonrangeCal(snowts, LSTts, yt):\n",
    "    lastsnow = np.array((snowts.loc[yt]['mean'][:120]>50)).nonzero()[0]\n",
    "    startsnow = np.array((snowts.loc[yt]['mean'][-120:]>50)).nonzero()[0]\n",
    "    spring = np.array((LSTts.loc[yt]['mean'][:120]*0.02-273.15>0)).nonzero()[0]\n",
    "    winter = np.array((LSTts.loc[yt]['mean'][-120:]*0.02-273.15<0)).nonzero()[0]\n",
    "\n",
    "    if len(lastsnow)!=0:\n",
    "        start_season = snowts.loc[yt].index[lastsnow[-1]]\n",
    "    elif len(spring)!=0:\n",
    "        start_season = snowts.loc[yt].index[spring[0]]\n",
    "    else:\n",
    "        start_season = snowts.loc[yt].index[0]\n",
    "    if len(startsnow)!=0:\n",
    "        end_season = snowts.loc[yt].index[-120:][startsnow[0]]\n",
    "    elif len(winter)!=0:\n",
    "        end_season = snowts.loc[yt].index[-120:][winter[0]]\n",
    "    else: \n",
    "        end_season = snowts.loc[yt].index[-1]\n",
    "    return start_season, end_season\n",
    "\n",
    "def Season(roiall, siteID, y_start, y_end):\n",
    "    startL=[]\n",
    "    endL=[]\n",
    "    snow=ee.ImageCollection('MODIS/061/MOD10A1').filter(ee.Filter.date(f'{y_start}-01-01', f'{y_end}-01-01'))\n",
    "    snow=snow.map(snowmaskFunc)\n",
    "    snowts=snowdf(roiall, snow, siteID)\n",
    "\n",
    "    LST=ee.ImageCollection('MODIS/061/MOD11A1').filter(ee.Filter.date(f'{y_start}-01-01', f'{y_end}-01-01'))\n",
    "    LST=LST.map(LSTmaskFunc)\n",
    "    LSTts=LSTdf(roiall, LST, siteID)\n",
    "    Years = np.arange(y_start, y_end, 1)\n",
    "    for y in Years:\n",
    "        yt=f'{y}'\n",
    "        st,ed=seasonrangeCal(snowts, LSTts, yt)\n",
    "        #st,ed=seasonrangeCal(snowts,0 , yt)\n",
    "        startL.append(st)\n",
    "        endL.append(ed)\n",
    "    df=pd.DataFrame(data={'start_season' : startL, 'end_season' : endL}, index=Years)\n",
    "    return df, snowts, LSTts\n",
    "\n",
    "#cdl function\n",
    "def createTScdl(image):\n",
    "    date = image.get('system:time_start')\n",
    "    value = image.reduceRegion(reducer=ee.Reducer.median(), geometry=roi).get('cropland')\n",
    "    ft = ee.Feature(None, {'date': ee.Date(date).format('Y/M/d'), 'mean': value})\n",
    "    return ft\n",
    "\n",
    "def TS_to_pandascdl(TS):\n",
    "    dump = TS.getInfo()\n",
    "    fts = dump['features']\n",
    "    out_vals = np.empty((len(fts)))\n",
    "    out_dates = []\n",
    "    \n",
    "    for i, f in enumerate(fts):\n",
    "        props = f['properties']\n",
    "        date = props['date']\n",
    "        if len(f['properties'])==1:\n",
    "            val = 0\n",
    "        else:\n",
    "            val = props['mean']\n",
    "        out_vals[i] = val\n",
    "        out_dates.append(pd.Timestamp(date))\n",
    "    \n",
    "    df = pd.DataFrame({'crop' : out_vals}, index=out_dates)\n",
    "    return df\n",
    "\n",
    "def cdldf(cdl, siteID):\n",
    "    coords = roiall.getInfo()['features'][siteID]['geometry']['coordinates']\n",
    "    global roi\n",
    "    roi = ee.Geometry.MultiPolygon(coords)\n",
    "    \n",
    "    cropLandcover = cdl.select('cropland').sort('system:time_start').filterBounds(roi).map(Getroi)\n",
    "    ts=cropLandcover.map(createTScdl)\n",
    "    df=TS_to_pandascdl(ts)\n",
    "    df.index=df.index.year\n",
    "    return df\n",
    "\n",
    "#sentinel 1 function\n",
    "def createTSS1(image):\n",
    "    date = image.get('system:time_start')\n",
    "    orb = image.get('relativeOrbitNumber_start')\n",
    "    slice = image.get('sliceNumber')\n",
    "    VVvalue = image.reduceRegion(reducer=ee.Reducer.mean(), geometry=roi).get('VV')\n",
    "    VVmx = image.reduceRegion(reducer=ee.Reducer.max(), geometry=roi).get('VV')\n",
    "    VHvalue = image.reduceRegion(reducer=ee.Reducer.mean(), geometry=roi).get('VH')\n",
    "    VHmx = image.reduceRegion(reducer=ee.Reducer.max(), geometry=roi).get('VH')\n",
    "    ft = ee.Feature(None, {'date': ee.Date(date).format('Y/M/d'), 'VVmean': VVvalue, 'VVmax':VVmx, 'VHmean': VHvalue\n",
    "                           , 'VHmax':VHmx, 'orbit':orb, 'slice':slice})\n",
    "    return ft\n",
    "\n",
    "def TS_to_pandass1(TS):\n",
    "    dump = TS.getInfo()\n",
    "    fts = dump['features']\n",
    "    out_vals = np.empty((len(fts)))\n",
    "    out_vals2 = np.empty((len(fts)))\n",
    "    out_vals3 = np.empty((len(fts)))\n",
    "    out_vals4 = np.empty((len(fts)))\n",
    "    out_dates = []\n",
    "    out_orbits = []\n",
    "    out_slices = []\n",
    "    \n",
    "    for i, f in enumerate(fts):\n",
    "        props = f['properties']\n",
    "        date = props['date']\n",
    "        val = props['VVmean']\n",
    "        out_vals[i] = val\n",
    "        val = props['VVmax']\n",
    "        out_vals2[i] = val\n",
    "        val = props['VHmean']\n",
    "        out_vals3[i] = val\n",
    "        val = props['VHmax']\n",
    "        out_vals4[i] = val\n",
    "        out_dates.append(pd.Timestamp(date))\n",
    "        out_orbits.append(props['orbit'])\n",
    "        out_slices.append(props['slice'])\n",
    "    \n",
    "    df = pd.DataFrame({'VVmean' : out_vals, 'VVmax': out_vals2, 'VHmean' : out_vals3, 'VHmax': out_vals4\n",
    "                       , 'orbit':out_orbits, 'slice':out_slices}, index=out_dates)\n",
    "    return df\n",
    "\n",
    "def s1data(y_start, y_end, siteID, direction = 'Ascending', orbit = None, dataset='FLOAT'):\n",
    "    coords = roiall.getInfo()['features'][siteID]['geometry']['coordinates']\n",
    "    global roi, var\n",
    "    roi = ee.Geometry.MultiPolygon(coords)\n",
    "    if dataset == 'FLOAT':\n",
    "        S1 = ee.ImageCollection('COPERNICUS/S1_GRD_FLOAT') \n",
    "    elif dataset == 'LOG':\n",
    "        S1 = ee.ImageCollection('COPERNICUS/S1_GRD')#log-scaled (in dB)\n",
    "    S1 = S1.filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV'))\\\n",
    "        .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VH'))\\\n",
    "        .filter(ee.Filter.eq('instrumentMode', 'IW'))\\\n",
    "        .filterBounds(roi)\\\n",
    "        .filter(ee.Filter.date(f'{y_start}-01-01', f'{y_end}-01-01'))\n",
    "\n",
    "    if orbit:\n",
    "        S1 = S1.filter(ee.Filter.eq('relativeOrbitNumber_start', orbit))\n",
    "    if direction == 'Ascending':\n",
    "        data = S1.filter(ee.Filter.eq('orbitProperties_pass', 'ASCENDING'))\n",
    "    else:\n",
    "        data = S1.filter(ee.Filter.eq('orbitProperties_pass', 'DESCENDING'))\n",
    "    data = data.map(Getroi)\n",
    "    data = data.map(createTSS1)\n",
    "    df = TS_to_pandass1(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define variables\n",
    "roi_shp = 'Source/masklayers/potato_flt_mask.shp'\n",
    "roiall = geemap.shp_to_ee(roi_shp)\n",
    "cdl=ee.ImageCollection('USDA/NASS/CDL').filter(ee.Filter.date('2000-01-01', '2023-01-01'))\n",
    "NDVICollection = ee.ImageCollection('MODIS/061/MCD43A4')\n",
    "var = 'NDVI'\n",
    "scale_factor = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites_pth = 'Source/Data/sites'\n",
    "cdl_pth = 'Source/Data/cdl'\n",
    "season_pth = 'Source/Data/season'\n",
    "sentinel1_pth = 'Source/Data/sentinel1'\n",
    "\n",
    "if not os.path.exists(sites_pth):\n",
    "    os.makedirs(sites_pth)\n",
    "if not os.path.exists(cdl_pth):\n",
    "    os.makedirs(cdl_pth)\n",
    "if not os.path.exists(season_pth):\n",
    "    os.makedirs(season_pth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Vito\\miniconda3\\envs\\gee\\Lib\\site-packages\\pyproj\\crs\\crs.py:141: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  in_crs_string = _prepare_from_proj_string(in_crs_string)\n",
      "c:\\Users\\Vito\\miniconda3\\envs\\gee\\Lib\\site-packages\\pyproj\\crs\\crs.py:141: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  in_crs_string = _prepare_from_proj_string(in_crs_string)\n",
      "C:\\Users\\Vito\\AppData\\Local\\Temp\\ipykernel_14712\\3325219268.py:16: FutureWarning: This function is deprecated. See: https://pyproj4.github.io/pyproj/stable/gotchas.html#upgrading-to-pyproj-2-from-pyproj-1\n",
      "  lon, lat = transform(source_projection, target_projection, x, y)\n"
     ]
    }
   ],
   "source": [
    "source_projection = Proj(init='epsg:5070')\n",
    "target_projection = Proj(init='epsg:4326')  # WGS84\n",
    "\n",
    "gdf = gpd.read_file(roi_shp)\n",
    "\n",
    "geometry = gdf['geometry']\n",
    "epsgcenterlist=[]\n",
    "wgscenterlist=[]\n",
    "for geom in geometry:\n",
    "    x, y = geom.centroid.x, geom.centroid.y\n",
    "    lon, lat = transform(source_projection, target_projection, x, y)\n",
    "    epsgcenterlist.append([x, y])\n",
    "    wgscenterlist.append([lon, lat])\n",
    "np.save('Soucr/masklayers/wgscenterlist.npy', wgscenterlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "Failed sites: []\n"
     ]
    }
   ],
   "source": [
    "#sentinel 1 donwload\n",
    "if not os.path.exists(sentinel1_pth):\n",
    "    os.makedirs(sentinel1_pth)\n",
    "failed_sites = []\n",
    "for i in np.arange(0,148,1):\n",
    "    templist=[]\n",
    "    try:\n",
    "        df = s1data(2015, 2023, i, direction = 'Decending', orbit = None, dataset='LOG')\n",
    "        df.to_hdf(sentinel1_pth+f'/Site{i:03}_db.h5', key='df', mode='w') \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: download failed for site {i}\")\n",
    "        failed_sites.append(i)\n",
    "print(f\"Failed sites: {failed_sites}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "site 12 26\n",
    "27\n",
    "73\n",
    "89\n",
    "works when split in smaller batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NDVI download\n",
    "failed_sites = []\n",
    "for i in np.arange(0,148,1):\n",
    "    templist=[]\n",
    "    for yt in np.arange(2000,2023,1):\n",
    "        try:\n",
    "            templist.append(getsiteNDVI(roiall, NDVICollection, i, f'{yt}-01-01', f'{yt+1}-01-01', 30))\n",
    "        except Exception as e:\n",
    "            print(f\"Error: download failed for site {i} year {yt}\")\n",
    "            failed_sites.append(f'{i}_{yt}')\n",
    "    St=pd.concat(templist, axis=0, join='inner')\n",
    "    St.to_hdf(sites_pth+f'/Site{i:03}_NBARint.h5', key='df', mode='w')\n",
    "print(f\"Failed sites: {failed_sites}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NDVI download\n",
    "failed_sites = []\n",
    "for i in [12,26,27,73,89]:\n",
    "    templist=[]\n",
    "    for yt in np.arange(2000,2023,1):\n",
    "        try:\n",
    "            templist.append(getsiteNDVI(roiall, NDVICollection, i, f'{yt}-01-01', f'{yt}-04-01', 30))\n",
    "            templist.append(getsiteNDVI(roiall, NDVICollection, i, f'{yt}-04-01', f'{yt}-07-01', 30))\n",
    "            templist.append(getsiteNDVI(roiall, NDVICollection, i, f'{yt}-07-01', f'{yt}-10-01', 30))\n",
    "            templist.append(getsiteNDVI(roiall, NDVICollection, i, f'{yt}-10-01', f'{yt+1}-01-01', 30))\n",
    "        except Exception as e:\n",
    "            print(f\"Error: download failed for site {i} year {yt}\")\n",
    "            failed_sites.append(f'{i}_{yt}')\n",
    "    St=pd.concat(templist, axis=0, join='inner')\n",
    "    St.to_hdf(sites_pth+f'/Site{i:03}_NBARint.h5', key='df', mode='w')\n",
    "print(f\"Failed sites: {failed_sites}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "problem for season download\n",
    "site 12\n",
    "26\n",
    "73\n",
    "79\n",
    "89\n",
    "works after spliting in 5 years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "Error: download failed for site 73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "Error: download failed for site 79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "Error: download failed for site 89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n"
     ]
    }
   ],
   "source": [
    "#NDSI season download\n",
    "failed_sites = []\n",
    "for i in np.arange(0,148,1):\n",
    "    try:\n",
    "        d1,snow1,lst1=Season(roiall, i, 2000, 2013)\n",
    "        d2,snow2,lst2=Season(roiall, i, 2013, 2023)\n",
    "        St1=pd.concat([d1,d2], axis=0, join='inner')\n",
    "        St1.to_hdf(season_pth+f'/Site{i:03}_season_day.h5', key='df', mode='w')\n",
    "        St2=pd.concat([snow1,snow2], axis=0, join='inner')\n",
    "        St2.to_hdf(season_pth+f'/Site{i:03}_snow_ts.h5', key='df', mode='w') \n",
    "        St3=pd.concat([lst1,lst2], axis=0, join='inner')\n",
    "        St3.to_hdf(season_pth+f'/Site{i:03}_lst_ts.h5', key='df', mode='w') \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: download failed for site {i}\")\n",
    "        failed_sites.append(i)\n",
    "print(f\"Failed sites: {failed_sites}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "26\n",
      "73\n",
      "79\n",
      "89\n",
      "Failed sites: []\n"
     ]
    }
   ],
   "source": [
    "#season download splited\n",
    "failed_sites = []\n",
    "for i in [12,26,73,79,89]:\n",
    "    try:\n",
    "        d1,snow1,lst1=Season(roiall, i, 2000, 2005)\n",
    "        d2,snow2,lst2=Season(roiall, i, 2005, 2010)\n",
    "        d3,snow3,lst3=Season(roiall, i, 2010, 2015)\n",
    "        d4,snow4,lst4=Season(roiall, i, 2015, 2020)\n",
    "        d5,snow5,lst5=Season(roiall, i, 2020, 2023)\n",
    "        \n",
    "        St1=pd.concat([d1,d2,d3,d4,d5], axis=0, join='inner')\n",
    "        St1.to_hdf(season_pth+f'/Site{i:03}_season_day.h5', key='df', mode='w')\n",
    "        St2=pd.concat([snow1,snow2,snow3,snow4,snow5], axis=0, join='inner')\n",
    "        St2.to_hdf(season_pth+f'/Site{i:03}_snow_ts.h5', key='df', mode='w') \n",
    "        St3=pd.concat([lst1,lst2,lst3,lst4,lst5], axis=0, join='inner')\n",
    "        St3.to_hdf(season_pth+f'/Site{i:03}_lst_ts.h5', key='df', mode='w') \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: download failed for site {i}\")\n",
    "        failed_sites.append(i)\n",
    "print(f\"Failed sites: {failed_sites}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n"
     ]
    }
   ],
   "source": [
    "for i in np.arange(0,148,1):\n",
    "    temp=cdldf(cdl, i)\n",
    "    temp.to_hdf(cdl_path+f'/Site{i:03}_cdl.h5', key='df', mode='w') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gee",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
