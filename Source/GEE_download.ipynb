{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import geemap\n",
    "import ee\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from pyproj import Proj, transform\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization successful\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    ee.Initialize()\n",
    "    print(\"Initialization successful\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: authentication needed\")\n",
    "    ee.Authenticate()\n",
    "    ee.Initialize()\n",
    "    print(\"Initialization successful\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TS function\n",
    "def dailyNBARmaskFunc(img):\n",
    "    qa = img.select('BRDF_Albedo_Band_Mandatory_Quality_Band1')\n",
    "    qa2 = img.select('BRDF_Albedo_Band_Mandatory_Quality_Band2')\n",
    "    Quality = bitwiseExtract(qa, 0)\n",
    "    Qualityb2 = bitwiseExtract(qa2, 0)\n",
    "    mask = Quality.eq(0)\\\n",
    "        .And(Qualityb2.eq(0))\\\n",
    "\n",
    "\n",
    "    maskedImage = img.updateMask(mask)\n",
    "\n",
    "    return maskedImage\n",
    "\n",
    "def dailyNBARNDVI(img):\n",
    "    ndvi = img.normalizedDifference(['Nadir_Reflectance_Band2', 'Nadir_Reflectance_Band1']).rename('NDVI').set('system:time_start', img.get('system:time_start'))\n",
    "    return img.addBands(ndvi)\n",
    "\n",
    "def bitwiseExtract(value, fromBit, toBit=None):\n",
    "    '''\n",
    "    https://gis.stackexchange.com/questions/349371/creating-cloud-free-images-out-of-a-mod09a1-modis-image-in-gee/349401#349401\n",
    "    '''\n",
    "    if toBit == None:\n",
    "        toBit = fromBit\n",
    "    maskSize = ee.Number(1).add(toBit).subtract(fromBit)\n",
    "    mask = ee.Number(1).leftShift(maskSize).subtract(1)\n",
    "    return value.rightShift(fromBit).bitwiseAnd(mask)\n",
    "\n",
    "def Getroi(img):\n",
    "    \n",
    "    maskedImage = img.clip(roi)\n",
    "    \n",
    "    return maskedImage\n",
    "def rescale(image):\n",
    "    date = image.get('system:time_start')\n",
    "    return image.multiply(scale_factor).set('system:time_start', date)\n",
    "\n",
    "def createTS(image):\n",
    "    date = image.get('system:time_start')\n",
    "    value = image.reduceRegion(reducer=ee.Reducer.mean(), geometry=roi).get(var)\n",
    "    std = image.reduceRegion(reducer=ee.Reducer.stdDev(), geometry=roi).get(var)\n",
    "    ft = ee.Feature(None, {'date': ee.Date(date).format('Y/M/d'), var: value, 'STD': std})\n",
    "    return ft\n",
    "\n",
    "def TS_to_pandas(TS):\n",
    "    dump = TS.getInfo()\n",
    "    fts = dump['features']\n",
    "    out_vals = np.empty((len(fts)))\n",
    "    out_dates = []\n",
    "    out_std = np.empty((len(fts)))\n",
    "    \n",
    "    for i, f in enumerate(fts):\n",
    "        props = f['properties']\n",
    "        date = props['date']\n",
    "        val = props[var]\n",
    "        std = props['STD']\n",
    "        out_vals[i] = val\n",
    "        out_std[i] = std\n",
    "        out_dates.append(pd.Timestamp(date))\n",
    "    \n",
    "    df = pd.DataFrame({'mean' : out_vals, 'std' : out_std}, index=out_dates)\n",
    "    return df\n",
    "\n",
    "\n",
    "#GEE interpolation cloud comp\n",
    "#credit https://spatialthoughts.com/2021/11/08/temporal-interpolation-gee/\n",
    "def interpolate(image):\n",
    "    image = ee.Image(image)\n",
    "    date = image.get('system:time_start')\n",
    "    beforeImages = ee.List(image.get('before'))\n",
    "    beforeMosaic = ee.ImageCollection.fromImages(beforeImages).mosaic()\n",
    "    afterImages = ee.List(image.get('after'))\n",
    "    afterMosaic = ee.ImageCollection.fromImages(afterImages).mosaic()\n",
    "    \n",
    "    t1 = beforeMosaic.select('timestamp').rename('t1')\n",
    "    t2 = afterMosaic.select('timestamp').rename('t2')\n",
    "    t = image.metadata('system:time_start').rename('t')\n",
    "    tImage = ee.Image.cat([t1, t2, t])\n",
    "    timeRatio = tImage.expression('(t - t1) / (t2 - t1)', {'t': tImage.select('t'),'t1': tImage.select('t1'),'t2': tImage.select('t2')})\n",
    "\n",
    "    interpolated = beforeMosaic.add((afterMosaic.subtract(beforeMosaic).multiply(timeRatio)))\n",
    "    result = image.unmask(interpolated)\n",
    "    return result.copyProperties(image, ['system:time_start'])\n",
    "\n",
    "def timeImage(image):\n",
    "    tI = image.metadata('system:time_start').rename('timestamp')\n",
    "    timeImageMasked = tI.updateMask(image.mask().select(14))\n",
    "    return image.select('NDVI').addBands(timeImageMasked)\n",
    "\n",
    "def getsiteNDVI(roiall, NDVICollection, siteID, y_start, y_end, n):\n",
    "    coords = roiall.getInfo()['features'][siteID]['geometry']['coordinates']\n",
    "    global roi\n",
    "    roi = ee.Geometry.MultiPolygon(coords)\n",
    "\n",
    "    NDVI_flt = NDVICollection.filter(ee.Filter.date(y_start, y_end))\n",
    "    NDVImasked = NDVI_flt.map(dailyNBARmaskFunc)\n",
    "    NDVI_roimasked = NDVImasked.map(Getroi)\n",
    "\n",
    "    NDVI_rescale = NDVI_roimasked.map(rescale)\n",
    "    NDVI_rescale = NDVI_rescale.map(dailyNBARNDVI)\n",
    "    #pixel based interpolation\n",
    "    days = n\n",
    "\n",
    "    millis = ee.Number(days).multiply(1000*60*60*24)\n",
    "\n",
    "    NDVI_rescale_t = NDVI_rescale.map(timeImage)\n",
    "\n",
    "    maxDiffFilter = ee.Filter.maxDifference(**{'difference': millis,'leftField': 'system:time_start','rightField': 'system:time_start'})\n",
    "\n",
    "    lessEqFilter = ee.Filter.lessThanOrEquals(**{'leftField': 'system:time_start','rightField': 'system:time_start'})\n",
    "\n",
    "    greaterEqFilter = ee.Filter.greaterThanOrEquals(**{'leftField': 'system:time_start','rightField': 'system:time_start'})\n",
    "\n",
    "    filter1 = ee.Filter.And(maxDiffFilter, lessEqFilter)\n",
    "\n",
    "    join1 = ee.Join.saveAll(**{'matchesKey': 'after','ordering': 'system:time_start','ascending': False})\n",
    "\n",
    "    join1Result = join1.apply(**{'primary': NDVI_rescale_t,'secondary': NDVI_rescale_t,'condition': filter1})\n",
    "\n",
    "    filter2 = ee.Filter.And(maxDiffFilter, greaterEqFilter)\n",
    "\n",
    "    join2 = ee.Join.saveAll(**{'matchesKey': 'before','ordering': 'system:time_start','ascending': True})\n",
    "\n",
    "    join2Result = join2.apply(**{'primary':join1Result, 'secondary': join1Result, 'condition': filter2})\n",
    "\n",
    "    # Map the interpolation function over the image collection\n",
    "    interpolated_collection = ee.ImageCollection(join2Result.map(interpolate))\n",
    "    #interpolated_collection_ndvi =interpolated_collection.select(['NDVI']).map(lambda img:img.multiply(1).copyProperties(img, **{'properties':['system:time_start', 'system:index']}))\n",
    "    TS = interpolated_collection.map(createTS)\n",
    "    NDVI_ts_int = TS_to_pandas(TS)\n",
    "    return NDVI_ts_int\n",
    "\n",
    "#Season functions\n",
    "\n",
    "def snowdf(roiall,snow, siteID):\n",
    "    coords = roiall.getInfo()['features'][siteID]['geometry']['coordinates']\n",
    "    global roi, var\n",
    "    roi = ee.Geometry.MultiPolygon(coords)\n",
    "    var = 'NDSI_Snow_Cover'\n",
    "\n",
    "    snowcover = snow.select('NDSI_Snow_Cover').sort('system:time_start').filterBounds(roi).map(Getroi)\n",
    "    ts=snowcover.map(createTS)\n",
    "    df=TS_to_pandas(ts)\n",
    "    return df\n",
    "\n",
    "def snowmaskFunc(img):\n",
    "    qa = img.select('NDSI_Snow_Cover_Basic_QA')\n",
    "    Quality = bitwiseExtract(qa, 0, 15) \n",
    "    mask = Quality.eq(0).Or(Quality.eq(1))\n",
    "\n",
    "\n",
    "    maskedImage = img.updateMask(mask)\n",
    "\n",
    "    return maskedImage\n",
    "\n",
    "\n",
    "def LSTdf(roiall, LST, siteID):\n",
    "    coords = roiall.getInfo()['features'][siteID]['geometry']['coordinates']\n",
    "    global roi, var\n",
    "    roi = ee.Geometry.MultiPolygon(coords)\n",
    "    var = 'LST_Day_1km'\n",
    "    \n",
    "    LSTC = LST.select(var).sort('system:time_start').filterBounds(roi).map(Getroi)\n",
    "    ts=LSTC.map(createTS)\n",
    "    df=TS_to_pandas(ts)\n",
    "    return df\n",
    "\n",
    "def LSTmaskFunc(img):\n",
    "    qa = img.select('QC_Day')\n",
    "    Quality = bitwiseExtract(qa, 0, 1)\n",
    "    Quality2 = bitwiseExtract(qa, 2, 3) \n",
    "    mask = Quality.eq(0).Or(Quality2.eq(0))\n",
    "\n",
    "    maskedImage = img.updateMask(mask)\n",
    "\n",
    "    return maskedImage\n",
    "\n",
    "def seasonrangeCal(snowts, LSTts, yt):\n",
    "    lastsnow = np.array((snowts.loc[yt]['mean'][:120]>50)).nonzero()[0]\n",
    "    startsnow = np.array((snowts.loc[yt]['mean'][-120:]>50)).nonzero()[0]\n",
    "    spring = np.array((LSTts.loc[yt]['mean'][:120]*0.02-273.15>0)).nonzero()[0]\n",
    "    winter = np.array((LSTts.loc[yt]['mean'][-120:]*0.02-273.15<0)).nonzero()[0]\n",
    "\n",
    "    if len(lastsnow)!=0:\n",
    "        start_season = snowts.loc[yt].index[lastsnow[-1]]\n",
    "    elif len(spring)!=0:\n",
    "        start_season = snowts.loc[yt].index[spring[0]]\n",
    "    else:\n",
    "        start_season = snowts.loc[yt].index[0]\n",
    "    if len(startsnow)!=0:\n",
    "        end_season = snowts.loc[yt].index[-120:][startsnow[0]]\n",
    "    elif len(winter)!=0:\n",
    "        end_season = snowts.loc[yt].index[-120:][winter[0]]\n",
    "    else: \n",
    "        end_season = snowts.loc[yt].index[-1]\n",
    "    return start_season, end_season\n",
    "\n",
    "def Season(roiall, siteID, y_start, y_end):\n",
    "    startL=[]\n",
    "    endL=[]\n",
    "    snow=ee.ImageCollection('MODIS/061/MOD10A1').filter(ee.Filter.date(f'{y_start}-01-01', f'{y_end}-01-01'))\n",
    "    snow=snow.map(snowmaskFunc)\n",
    "    snowts=snowdf(roiall, snow, siteID)\n",
    "\n",
    "    LST=ee.ImageCollection('MODIS/061/MOD11A1').filter(ee.Filter.date(f'{y_start}-01-01', f'{y_end}-01-01'))\n",
    "    LST=LST.map(LSTmaskFunc)\n",
    "    LSTts=LSTdf(roiall, LST, siteID)\n",
    "    Years = np.arange(y_start, y_end, 1)\n",
    "    for y in Years:\n",
    "        yt=f'{y}'\n",
    "        st,ed=seasonrangeCal(snowts, LSTts, yt)\n",
    "        #st,ed=seasonrangeCal(snowts,0 , yt)\n",
    "        startL.append(st)\n",
    "        endL.append(ed)\n",
    "    df=pd.DataFrame(data={'start_season' : startL, 'end_season' : endL}, index=Years)\n",
    "    return df, snowts, LSTts\n",
    "\n",
    "#cdl function\n",
    "def createTScdl(image):\n",
    "    date = image.get('system:time_start')\n",
    "    value = image.reduceRegion(reducer=ee.Reducer.median(), geometry=roi).get('cropland')\n",
    "    ft = ee.Feature(None, {'date': ee.Date(date).format('Y/M/d'), 'mean': value})\n",
    "    return ft\n",
    "\n",
    "def TS_to_pandascdl(TS):\n",
    "    dump = TS.getInfo()\n",
    "    fts = dump['features']\n",
    "    out_vals = np.empty((len(fts)))\n",
    "    out_dates = []\n",
    "    \n",
    "    for i, f in enumerate(fts):\n",
    "        props = f['properties']\n",
    "        date = props['date']\n",
    "        if len(f['properties'])==1:\n",
    "            val = 0\n",
    "        else:\n",
    "            val = props['mean']\n",
    "        out_vals[i] = val\n",
    "        out_dates.append(pd.Timestamp(date))\n",
    "    \n",
    "    df = pd.DataFrame({'crop' : out_vals}, index=out_dates)\n",
    "    return df\n",
    "\n",
    "def cdldf(cdl, siteID):\n",
    "    coords = roiall.getInfo()['features'][siteID]['geometry']['coordinates']\n",
    "    global roi\n",
    "    roi = ee.Geometry.MultiPolygon(coords)\n",
    "    \n",
    "    cropLandcover = cdl.select('cropland').sort('system:time_start').filterBounds(roi).map(Getroi)\n",
    "    ts=cropLandcover.map(createTScdl)\n",
    "    df=TS_to_pandascdl(ts)\n",
    "    df.index=df.index.year\n",
    "    return df\n",
    "\n",
    "#sentinel 1 function\n",
    "def createTSS1(image):\n",
    "    date = image.get('system:time_start')\n",
    "    orb = image.get('relativeOrbitNumber_start')\n",
    "    slice = image.get('sliceNumber')\n",
    "    VVvalue = image.reduceRegion(reducer=ee.Reducer.mean(), geometry=roi).get('VV')\n",
    "    VVmx = image.reduceRegion(reducer=ee.Reducer.max(), geometry=roi).get('VV')\n",
    "    VHvalue = image.reduceRegion(reducer=ee.Reducer.mean(), geometry=roi).get('VH')\n",
    "    VHmx = image.reduceRegion(reducer=ee.Reducer.max(), geometry=roi).get('VH')\n",
    "    ft = ee.Feature(None, {'date': ee.Date(date).format('Y/M/d'), 'VVmean': VVvalue, 'VVmax':VVmx, 'VHmean': VHvalue\n",
    "                           , 'VHmax':VHmx, 'orbit':orb, 'slice':slice})\n",
    "    return ft\n",
    "\n",
    "def TS_to_pandass1(TS):\n",
    "    dump = TS.getInfo()\n",
    "    fts = dump['features']\n",
    "    out_vals = np.empty((len(fts)))\n",
    "    out_vals2 = np.empty((len(fts)))\n",
    "    out_vals3 = np.empty((len(fts)))\n",
    "    out_vals4 = np.empty((len(fts)))\n",
    "    out_dates = []\n",
    "    out_orbits = []\n",
    "    out_slices = []\n",
    "    \n",
    "    for i, f in enumerate(fts):\n",
    "        props = f['properties']\n",
    "        date = props['date']\n",
    "        val = props['VVmean']\n",
    "        out_vals[i] = val\n",
    "        val = props['VVmax']\n",
    "        out_vals2[i] = val\n",
    "        val = props['VHmean']\n",
    "        out_vals3[i] = val\n",
    "        val = props['VHmax']\n",
    "        out_vals4[i] = val\n",
    "        out_dates.append(pd.Timestamp(date))\n",
    "        out_orbits.append(props['orbit'])\n",
    "        out_slices.append(props['slice'])\n",
    "    \n",
    "    df = pd.DataFrame({'VVmean' : out_vals, 'VVmax': out_vals2, 'VHmean' : out_vals3, 'VHmax': out_vals4\n",
    "                       , 'orbit':out_orbits, 'slice':out_slices}, index=out_dates)\n",
    "    return df\n",
    "\n",
    "def s1data(y_start, y_end, siteID, direction = 'Ascending', orbit = None, dataset='FLOAT'):\n",
    "    coords = roiall.getInfo()['features'][siteID]['geometry']['coordinates']\n",
    "    global roi, var\n",
    "    roi = ee.Geometry.MultiPolygon(coords)\n",
    "    if dataset == 'FLOAT':\n",
    "        S1 = ee.ImageCollection('COPERNICUS/S1_GRD_FLOAT') \n",
    "    elif dataset == 'LOG':\n",
    "        S1 = ee.ImageCollection('COPERNICUS/S1_GRD')#log-scaled (in dB)\n",
    "    S1 = S1.filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV'))\\\n",
    "        .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VH'))\\\n",
    "        .filter(ee.Filter.eq('instrumentMode', 'IW'))\\\n",
    "        .filterBounds(roi)\\\n",
    "        .filter(ee.Filter.date(f'{y_start}-01-01', f'{y_end}-01-01'))\n",
    "\n",
    "    if orbit:\n",
    "        S1 = S1.filter(ee.Filter.eq('relativeOrbitNumber_start', orbit))\n",
    "    if direction == 'Ascending':\n",
    "        data = S1.filter(ee.Filter.eq('orbitProperties_pass', 'ASCENDING'))\n",
    "    else:\n",
    "        data = S1.filter(ee.Filter.eq('orbitProperties_pass', 'DESCENDING'))\n",
    "    data = data.map(Getroi)\n",
    "    data = data.map(createTSS1)\n",
    "    df = TS_to_pandass1(data)\n",
    "    return df\n",
    "\n",
    "#CDL consistency function\n",
    "def dd(date):\n",
    "    return ee.Date(date).format('Y-MM-dd')\n",
    "def consist(ar):\n",
    "    if len(ar[1])!=0:\n",
    "        return np.max(ar[1])/np.sum(ar[1])\n",
    "    else:\n",
    "        return 0\n",
    "def warn_mark(i,n):\n",
    "    return f\"{i:04}_{n+2007}\"\n",
    "def warn_site_List(Lst):\n",
    "    return list(map(consist, Lst))\n",
    "def warn_site_years(Lst):\n",
    "    threshold=np.nonzero(np.array(Lst[1])<0.75)[0]\n",
    "    return list(map(lambda x: warn_mark(Lst[0], x), threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NorthDakota loaded\n",
      "Number of sites: 200\n"
     ]
    }
   ],
   "source": [
    "#Case='Idaho'\n",
    "Case='NorthDakota'\n",
    "#Case='Colorado'\n",
    "\n",
    "sites_pth = 'Data/'+Case+'/sites'\n",
    "cdl_pth = 'Data/'+Case+'/cdl'\n",
    "season_pth = 'Data/'+Case+'/season'\n",
    "sentinel1_pth = 'Data/'+Case+'/sentinel1'\n",
    "\n",
    "if not os.path.exists(sites_pth):\n",
    "    os.makedirs(sites_pth)\n",
    "if not os.path.exists(cdl_pth):\n",
    "    os.makedirs(cdl_pth)\n",
    "if not os.path.exists(season_pth):\n",
    "    os.makedirs(season_pth)\n",
    "if not os.path.exists(sentinel1_pth):\n",
    "    os.makedirs(sentinel1_pth)\n",
    "\n",
    "#define variables\n",
    "roi_shp = 'Data/'+Case+'/masklayers/site_mask.shp'\n",
    "roiall = geemap.shp_to_ee(roi_shp)\n",
    "cdl=ee.ImageCollection('USDA/NASS/CDL').filter(ee.Filter.date('2008-01-01', '2023-01-01'))\n",
    "NDVICollection = ee.ImageCollection('MODIS/061/MCD43A4').filter(ee.Filter.date('2000-01-01', '2023-01-01'))\n",
    "var = 'NDVI'\n",
    "scale_factor = 0.0001\n",
    "\n",
    "gdf = gpd.read_file(roi_shp)\n",
    "geometry = gdf['geometry']\n",
    "num_sites=len(geometry)\n",
    "print(Case+' loaded')\n",
    "print(f\"Number of sites: {num_sites}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No missing files in Data/NorthDakota/sites\n",
      "No missing files in Data/NorthDakota/cdl\n",
      "Missing files in Data/NorthDakota/season\n",
      "600\n",
      "Missing files in Data/NorthDakota/sentinel1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for set in [sites_pth, cdl_pth, season_pth, sentinel1_pth]:\n",
    "    file_names = os.listdir(set)\n",
    "    if len(file_names)!=num_sites:\n",
    "        print(f\"Missing files in {set}\")\n",
    "        print(len(file_names))\n",
    "    else:\n",
    "        print(f\"No missing files in {set}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading NorthDakota CDL Consistency: 100%|██████████| 200/200 [1:17:04<00:00, 23.12s/site]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of multicrop cases:  688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "AlldistL=[]\n",
    "for i in tqdm(np.arange(0,num_sites,1),'Downloading '+Case+' CDL Consistency', unit='site'):\n",
    "    coords = roiall.getInfo()['features'][i]['geometry']['coordinates']\n",
    "    roi = ee.Geometry.MultiPolygon(coords)\n",
    "\n",
    "    #create image collection of interested time frame and area and sort by date\n",
    "    im = ((cdl.filterBounds(roi)).map(Getroi)).sort('system:time_start')\n",
    "    #get date list\n",
    "    Acqt=np.sort(im.aggregate_array('system:time_start').map(dd).getInfo())\n",
    "    #convert to list for indexing\n",
    "    imL = im.toList(im.size())\n",
    "\n",
    "    distL=[]\n",
    "    for i in range(Acqt.size):\n",
    "        image=ee.Image(imL.get(i))\n",
    "        imar=ee.ImageCollection(image).getRegion(roi, scale=30).getInfo()\n",
    "        df = pd.DataFrame(imar[1:], columns=imar[0])\n",
    "        #if df['cropland'].notnull().any():\n",
    "        dist=np.array(np.unique(df['cropland'], return_counts=True))\n",
    "        distL.append(dist)\n",
    "    AlldistL.append(distL)\n",
    "site_consistencyL=list(map(warn_site_List, AlldistL))\n",
    "Warn=list(map(warn_site_years, enumerate(site_consistencyL)))\n",
    "\n",
    "count=0\n",
    "for i in Warn:\n",
    "    count+=len(i)\n",
    "print('Total number of multicrop cases: ', count)\n",
    "\n",
    "with open(cdl_pth+\"/CaseID_anom\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(Warn, fp)\n",
    "with open(cdl_pth+\"/ConsistencyPerc\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(site_consistencyL, fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of missing data cases:  0\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for i in range(len(AlldistL)):\n",
    "    lst=AlldistL[i]\n",
    "    for j in range(len(lst)):\n",
    "        ar=lst[j]\n",
    "        if len(ar[0])==0:\n",
    "            print(f\"Site {i} has no data for year {j+2007}\")\n",
    "            count+=1\n",
    "print('Total number of missing data cases: ', count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0000', '0001', '0002', '0003', '0004', '0005', '0006', '0007',\n",
       "       '0008', '0009', '0010', '0011', '0012', '0013', '0014', '0015',\n",
       "       '0016', '0017', '0018', '0019', '0020', '0021', '0022', '0023',\n",
       "       '0024', '0025', '0026', '0027', '0029', '0030', '0031', '0032',\n",
       "       '0033', '0034', '0035', '0036', '0037', '0038', '0039', '0040',\n",
       "       '0041', '0042', '0043', '0044', '0045', '0046', '0047', '0049',\n",
       "       '0050', '0051', '0052', '0053', '0054', '0055', '0056', '0057',\n",
       "       '0058', '0059', '0060', '0061', '0062', '0063', '0064', '0067',\n",
       "       '0068', '0069', '0070', '0071', '0072', '0073', '0075', '0076',\n",
       "       '0078', '0079', '0080', '0081', '0082', '0083', '0084', '0085',\n",
       "       '0087', '0088', '0089', '0090', '0091', '0092', '0094', '0095',\n",
       "       '0096', '0097', '0098', '0099', '0100', '0101', '0102', '0103',\n",
       "       '0104', '0105', '0106', '0107', '0108', '0109', '0110', '0111',\n",
       "       '0112', '0114', '0115', '0116', '0118', '0119', '0121', '0122',\n",
       "       '0123', '0125', '0126', '0127', '0128', '0129', '0130', '0131',\n",
       "       '0132', '0133', '0134', '0135', '0136', '0137', '0138', '0139',\n",
       "       '0140', '0142', '0143', '0145', '0146', '0147', '0148', '0150',\n",
       "       '0151', '0152', '0153', '0154', '0155', '0157', '0158', '0159',\n",
       "       '0160', '0161', '0162', '0163', '0164', '0165', '0166', '0167',\n",
       "       '0170', '0171', '0172', '0173', '0174', '0176', '0177', '0178',\n",
       "       '0179', '0180', '0181', '0182', '0185', '0186', '0188', '0189',\n",
       "       '0190', '0191', '0192', '0193', '0194', '0195', '0196', '0197',\n",
       "       '0199'], dtype='<U4')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Site 0000 has 7 problems\n",
      "Site 0001 has 1 problems\n",
      "Site 0002 has 1 problems\n",
      "Site 0003 has 7 problems\n",
      "Site 0004 has 3 problems\n",
      "Site 0005 has 1 problems\n",
      "Site 0006 has 3 problems\n",
      "Site 0007 has 2 problems\n",
      "Site 0008 has 2 problems\n",
      "Site 0009 has 1 problems\n",
      "Site 0010 has 1 problems\n",
      "Site 0011 has 3 problems\n",
      "Site 0012 has 1 problems\n",
      "Site 0013 has 1 problems\n",
      "Site 0014 has 1 problems\n",
      "Site 0015 has 2 problems\n",
      "Site 0016 has 1 problems\n",
      "Site 0017 has 4 problems\n",
      "Site 0018 has 2 problems\n",
      "Site 0019 has 12 problems\n",
      "Site 0020 has 4 problems\n",
      "Site 0021 has 2 problems\n",
      "Site 0022 has 1 problems\n",
      "Site 0023 has 1 problems\n",
      "Site 0024 has 3 problems\n",
      "Site 0025 has 1 problems\n",
      "Site 0026 has 3 problems\n",
      "Site 0027 has 3 problems\n",
      "Site 0029 has 1 problems\n",
      "Site 0030 has 2 problems\n",
      "Site 0031 has 5 problems\n",
      "Site 0032 has 2 problems\n",
      "Site 0033 has 1 problems\n",
      "Site 0034 has 9 problems\n",
      "Site 0035 has 11 problems\n",
      "Site 0036 has 1 problems\n",
      "Site 0037 has 1 problems\n",
      "Site 0038 has 3 problems\n",
      "Site 0039 has 10 problems\n",
      "Site 0040 has 1 problems\n",
      "Site 0041 has 7 problems\n",
      "Site 0042 has 2 problems\n",
      "Site 0043 has 4 problems\n",
      "Site 0044 has 2 problems\n",
      "Site 0045 has 10 problems\n",
      "Site 0046 has 4 problems\n",
      "Site 0047 has 6 problems\n",
      "Site 0049 has 13 problems\n",
      "Site 0050 has 2 problems\n",
      "Site 0051 has 4 problems\n",
      "Site 0052 has 14 problems\n",
      "Site 0053 has 1 problems\n",
      "Site 0054 has 3 problems\n",
      "Site 0055 has 3 problems\n",
      "Site 0056 has 2 problems\n",
      "Site 0057 has 9 problems\n",
      "Site 0058 has 1 problems\n",
      "Site 0059 has 1 problems\n",
      "Site 0060 has 1 problems\n",
      "Site 0061 has 2 problems\n",
      "Site 0062 has 1 problems\n",
      "Site 0063 has 9 problems\n",
      "Site 0064 has 3 problems\n",
      "Site 0067 has 3 problems\n",
      "Site 0068 has 9 problems\n",
      "Site 0069 has 9 problems\n",
      "Site 0070 has 3 problems\n",
      "Site 0071 has 10 problems\n",
      "Site 0072 has 2 problems\n",
      "Site 0073 has 4 problems\n",
      "Site 0075 has 4 problems\n",
      "Site 0076 has 8 problems\n",
      "Site 0078 has 1 problems\n",
      "Site 0079 has 1 problems\n",
      "Site 0080 has 2 problems\n",
      "Site 0081 has 2 problems\n",
      "Site 0082 has 8 problems\n",
      "Site 0083 has 3 problems\n",
      "Site 0084 has 5 problems\n",
      "Site 0085 has 3 problems\n",
      "Site 0087 has 3 problems\n",
      "Site 0088 has 7 problems\n",
      "Site 0089 has 1 problems\n",
      "Site 0090 has 2 problems\n",
      "Site 0091 has 5 problems\n",
      "Site 0092 has 3 problems\n",
      "Site 0094 has 9 problems\n",
      "Site 0095 has 10 problems\n",
      "Site 0096 has 2 problems\n",
      "Site 0097 has 1 problems\n",
      "Site 0098 has 12 problems\n",
      "Site 0099 has 1 problems\n",
      "Site 0100 has 2 problems\n",
      "Site 0101 has 1 problems\n",
      "Site 0102 has 2 problems\n",
      "Site 0103 has 2 problems\n",
      "Site 0104 has 1 problems\n",
      "Site 0105 has 13 problems\n",
      "Site 0106 has 10 problems\n",
      "Site 0107 has 3 problems\n",
      "Site 0108 has 5 problems\n",
      "Site 0109 has 1 problems\n",
      "Site 0110 has 9 problems\n",
      "Site 0111 has 2 problems\n",
      "Site 0112 has 6 problems\n",
      "Site 0114 has 10 problems\n",
      "Site 0115 has 12 problems\n",
      "Site 0116 has 9 problems\n",
      "Site 0118 has 1 problems\n",
      "Site 0119 has 1 problems\n",
      "Site 0121 has 1 problems\n",
      "Site 0122 has 2 problems\n",
      "Site 0123 has 2 problems\n",
      "Site 0125 has 3 problems\n",
      "Site 0126 has 1 problems\n",
      "Site 0127 has 2 problems\n",
      "Site 0128 has 10 problems\n",
      "Site 0129 has 11 problems\n",
      "Site 0130 has 1 problems\n",
      "Site 0131 has 1 problems\n",
      "Site 0132 has 5 problems\n",
      "Site 0133 has 2 problems\n",
      "Site 0134 has 2 problems\n",
      "Site 0135 has 7 problems\n",
      "Site 0136 has 1 problems\n",
      "Site 0137 has 10 problems\n",
      "Site 0138 has 9 problems\n",
      "Site 0139 has 2 problems\n",
      "Site 0140 has 1 problems\n",
      "Site 0142 has 1 problems\n",
      "Site 0143 has 2 problems\n",
      "Site 0145 has 3 problems\n",
      "Site 0146 has 8 problems\n",
      "Site 0147 has 2 problems\n",
      "Site 0148 has 1 problems\n",
      "Site 0150 has 3 problems\n",
      "Site 0151 has 1 problems\n",
      "Site 0152 has 1 problems\n",
      "Site 0153 has 7 problems\n",
      "Site 0154 has 4 problems\n",
      "Site 0155 has 2 problems\n",
      "Site 0157 has 3 problems\n",
      "Site 0158 has 1 problems\n",
      "Site 0159 has 11 problems\n",
      "Site 0160 has 5 problems\n",
      "Site 0161 has 1 problems\n",
      "Site 0162 has 2 problems\n",
      "Site 0163 has 1 problems\n",
      "Site 0164 has 3 problems\n",
      "Site 0165 has 2 problems\n",
      "Site 0166 has 5 problems\n",
      "Site 0167 has 9 problems\n",
      "Site 0170 has 4 problems\n",
      "Site 0171 has 2 problems\n",
      "Site 0172 has 1 problems\n",
      "Site 0173 has 11 problems\n",
      "Site 0174 has 1 problems\n",
      "Site 0176 has 2 problems\n",
      "Site 0177 has 3 problems\n",
      "Site 0178 has 1 problems\n",
      "Site 0179 has 2 problems\n",
      "Site 0180 has 5 problems\n",
      "Site 0181 has 1 problems\n",
      "Site 0182 has 2 problems\n",
      "Site 0185 has 4 problems\n",
      "Site 0186 has 11 problems\n",
      "Site 0188 has 2 problems\n",
      "Site 0189 has 10 problems\n",
      "Site 0190 has 1 problems\n",
      "Site 0191 has 1 problems\n",
      "Site 0192 has 4 problems\n",
      "Site 0193 has 3 problems\n",
      "Site 0194 has 1 problems\n",
      "Site 0195 has 1 problems\n",
      "Site 0196 has 4 problems\n",
      "Site 0197 has 1 problems\n",
      "Site 0199 has 11 problems\n"
     ]
    }
   ],
   "source": [
    "siteswithproblems=[]\n",
    "for i in np.unique(np.hstack(Warn)):\n",
    "    siteswithproblems.append(i[:4])\n",
    "temp=np.unique(siteswithproblems, return_counts=True)\n",
    "st,ct=temp[0],temp[1]\n",
    "for i,j in zip(st,ct):\n",
    "    print(f\"Site {i} has {j} problems\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "np.sum(ct>5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0052'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 177 artists>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAGdCAYAAACyzRGfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAhRElEQVR4nO3df5DU9X0/8Nfer+WH3CHID8+AB1P8SaIGkyZqWow/WkJoO5m2SUwJsbWDExO1dFKhNhVM5TRtHdJQaXQ6aCaD4Y9G64SmKdOqxKTJCGibpqmGFOUGZUhSPATigdzn+4ff3exxC3fHffZ9t/B4zHyGz773s+8fn5/P/eweW8iyLAsAgEQaRroDAMDpRfgAAJISPgCApIQPACAp4QMASEr4AACSEj4AgKSEDwAgqaaR7sCxent745VXXokJEyZEoVAY6e4AAIOQZVm8/vrr0d7eHg0NJ763MerCxyuvvBIzZswY6W4AACehq6sr3va2t51wmVEXPiZMmBARb3W+tbV1hHsDAAzG/v37Y8aMGeXr+ImMuvBR+qiltbVV+ACAOjOYr0z4wikAkJTwAQAkJXwAAEkJHwBAUsIHAJCU8AEAJCV8AABJCR8AQFLCBwCQlPABACQ15PCxZcuWWLRoUbS3t0ehUIjHH3/8uMsuXbo0CoVCrFmzZhhdBABOJUMOHwcPHoxLLrkk1q5de8LlHn/88fje974X7e3tJ905AODUM+QflluwYEEsWLDghMvs3r07PvWpT8U3v/nNWLhw4Ul3DgA49eT+q7a9vb2xePHi+MxnPhMXX3zxgMv39PRET09P+fH+/fvz7hIAMIrk/oXT++67L5qamuLWW28d1PKdnZ3R1tZWnmbMmJF3l6CsY/mm6Fi+aaS7AXBayzV8bNu2Lb7whS/Eww8/HIVCYVCvWbFiRXR3d5enrq6uPLsEAIwyuYaPb33rW7F3796YOXNmNDU1RVNTU7z88svxx3/8x9HR0VH1NcViMVpbW/tMAMCpK9fvfCxevDiuvfbaPmW/9mu/FosXL44bb7wxz6YAgDo15PBx4MCB2LFjR/nxzp074/nnn49JkybFzJkzY/LkyX2Wb25ujunTp8f5558//N4CAHVvyOFj69atcfXVV5cfL1u2LCIilixZEg8//HBuHQMATk1DDh/z58+PLMsGvfxLL7001CYAgFOY33YBAJISPgCApIQPACAp4QMASEr4AACSEj4AgKSEDwAgKeEDAEhK+AAAkhI+AICkhA8AICnhAwBISvgAAJISPgCApIQPACAp4QMASEr4AACSEj4AgKSED04pHcs3RcfyTSPdDUaAbQ/1Q/gAAJISPgCApIQPACAp4QMASEr4AACSEj4AgKSEDwAgKeEDAEhK+AAAkhI+AICkhA8AICnhAwBISvgAAJISPgCApIQPACAp4QMASEr4AACSEj4AgKSEDwAgKeEDAEhqyOFjy5YtsWjRomhvb49CoRCPP/54+bkjR47EHXfcEW9/+9tj/Pjx0d7eHh//+MfjlVdeybPPAEAdG3L4OHjwYFxyySWxdu3afs8dOnQotm/fHp/97Gdj+/bt8bWvfS1efPHF+I3f+I1cOgsA1L+mob5gwYIFsWDBgqrPtbW1xebNm/uUffGLX4x3v/vdsWvXrpg5c+bJ9RIAOGXU/Dsf3d3dUSgUYuLEibVuCgCoA0O+8zEUb7zxRixfvjxuuOGGaG1trbpMT09P9PT0lB/v37+/ll0CAEZYzcLHkSNH4iMf+Uj09vbGAw88cNzlOjs7Y9WqVbXqximnY/mm8vxL9y4cwZ5Afkr79Wjdp0d7/6De1ORjlyNHjsTv/u7vxs6dO2Pz5s3HvesREbFixYro7u4uT11dXbXoEgAwSuR+56MUPH70ox/Fk08+GZMnTz7h8sViMYrFYt7dAABGqSGHjwMHDsSOHTvKj3fu3BnPP/98TJo0Kdrb2+O3f/u3Y/v27fH1r389jh49Gnv27ImIiEmTJkVLS0t+PQcA6tKQw8fWrVvj6quvLj9etmxZREQsWbIkVq5cGU888URERFx66aV9Xvfkk0/G/PnzT76nAMApYcjhY/78+ZFl2XGfP9FzAAB+2wUASEr4AACSEj4AgKSEDwAgKeEDAEhK+AAAkhI+AICkhA8AICnhAwBISvgAAJISPgCApIQPACAp4QMASEr4AACSEj4AgKSEDwAgKeEDAEhK+AAAkhI+AICkhA9y07F8U3Qs3zTS3YC651j6hZFaF6naTT2+0bJvCR8AQFLCBwCQlPABACQlfAAASQkfAEBSwgcAkJTwAQAkJXwAAEkJHwBAUsIHAJCU8AEAJCV8AABJCR8AQFLCBwCQlPABACQlfAAASQkfAEBSwgcAkJTwAQAkJXwAAEkNOXxs2bIlFi1aFO3t7VEoFOLxxx/v83yWZbFy5cpob2+PsWPHxvz58+MHP/hBXv0FAOrckMPHwYMH45JLLom1a9dWff7zn/983H///bF27dp49tlnY/r06XHdddfF66+/PuzOAgD1r2moL1iwYEEsWLCg6nNZlsWaNWvizjvvjA996EMREfHII4/EtGnTYsOGDbF06dLh9RYAqHu5fudj586dsWfPnrj++uvLZcViMX71V381vvOd71R9TU9PT+zfv7/PBACcuoZ85+NE9uzZExER06ZN61M+bdq0ePnll6u+prOzM1atWpVnNxjlOpZvioiIl+5dmPS19eZUGmvqseTVXqmeVGrZ3qm0P53qTodtVZO/dikUCn0eZ1nWr6xkxYoV0d3dXZ66urpq0SUAYJTI9c7H9OnTI+KtOyBnn312uXzv3r397oaUFIvFKBaLeXYDABjFcr3zMWvWrJg+fXps3ry5XHb48OF4+umn44orrsizKQCgTg35zseBAwdix44d5cc7d+6M559/PiZNmhQzZ86M22+/PVavXh1z5syJOXPmxOrVq2PcuHFxww035NpxAKA+DTl8bN26Na6++ury42XLlkVExJIlS+Lhhx+OP/mTP4mf//zn8clPfjL27dsXv/zLvxz/8i//EhMmTMiv1wBA3Rpy+Jg/f35kWXbc5wuFQqxcuTJWrlw5nH4BAKcov+0CACQlfAAASQkfAEBSwgcAkJTwAQAkJXwAAEkJHwBAUsIHAJCU8AEAJCV8AABJCR8AQFLCBwCQlPABACQlfAAASQkfAEBSwgcAkJTwAQAkJXwAAEkJHwxLx/JN0bF800h3o2aGO74Tvb7yuVNxPdbDmE6mj/UwrhOp9/4P1+k+/tFC+AAAkhI+AICkhA8AICnhAwBISvgAAJISPgCApIQPACAp4QMASEr4AACSEj4AgKSEDwAgKeEDAEhK+AAAkhI+AICkhA8AICnhAwBISvgAAJISPgCApIQPACAp4QMASCr38PHmm2/Gn/3Zn8WsWbNi7NixMXv27Lj77rujt7c376YAgDrUlHeF9913X/zd3/1dPPLII3HxxRfH1q1b48Ybb4y2tra47bbb8m4OAKgzuYePf//3f4/f/M3fjIULF0ZEREdHRzz66KOxdevWvJsCAOpQ7h+7XHXVVfGv//qv8eKLL0ZExH/8x3/EM888Ex/4wAeqLt/T0xP79+/vMwEAp67c73zccccd0d3dHRdccEE0NjbG0aNH45577omPfvSjVZfv7OyMVatW5d0NOG10LN8UEREv3buwpq+BCPsO+cj9zsfGjRvjK1/5SmzYsCG2b98ejzzySPzVX/1VPPLII1WXX7FiRXR3d5enrq6uvLsEAIwiud/5+MxnPhPLly+Pj3zkIxER8fa3vz1efvnl6OzsjCVLlvRbvlgsRrFYzLsbAMAolfudj0OHDkVDQ99qGxsb/aktABARNbjzsWjRorjnnnti5syZcfHFF8dzzz0X999/f/z+7/9+3k0BAHUo9/DxxS9+MT772c/GJz/5ydi7d2+0t7fH0qVL48///M/zbgoAqEO5h48JEybEmjVrYs2aNXlXDQCcAvy2CwCQlPABACQlfAAASQkfAEBSwgcAkJTwAQAkJXwAAEkJHwBAUsIHAJCU8AEAJCV8AABJCR8AQFLCBwCQlPABACQlfAAASQkfAEBSwgcAkJTwAQAkddqFj47lm6Jj+aaTfp7asN5HP9uofozWbTVa+0V6p134AABGlvABACQlfAAASQkfAEBSwgcAkJTwAQAkJXwAAEkJHwBAUsIHAJCU8AEAJCV8AABJCR8AQFLCBwCQlPABACQlfAAASQkfAEBSwgcAkJTwAQAkJXwAAEkJHwBAUjUJH7t3747f+73fi8mTJ8e4cePi0ksvjW3bttWiKQCgzjTlXeG+ffviyiuvjKuvvjq+8Y1vxNSpU+PHP/5xTJw4Me+mAIA6lHv4uO+++2LGjBmxfv36cllHR0fezQAAdSr3j12eeOKJuPzyy+N3fud3YurUqXHZZZfFQw89dNzle3p6Yv/+/X0mAODUlXv4+N///d9Yt25dzJkzJ775zW/GzTffHLfeemt8+ctfrrp8Z2dntLW1lacZM2bk3SVOcR3LN0XH8k0j3Q1OEfYnqL3cw0dvb2+8853vjNWrV8dll10WS5cujT/8wz+MdevWVV1+xYoV0d3dXZ66urry7hIAMIrkHj7OPvvsuOiii/qUXXjhhbFr166qyxeLxWhtbe0zAQCnrtzDx5VXXhkvvPBCn7IXX3wxzj333LybAgDqUO7h44/+6I/iu9/9bqxevTp27NgRGzZsiAcffDBuueWWvJsCAOpQ7uHjXe96Vzz22GPx6KOPxty5c+Nzn/tcrFmzJj72sY/l3RQAUIdy/38+IiI++MEPxgc/+MFaVA0A1Dm/7QIAJCV8AABJCR8AQFLCBwCQlPABACQlfAAASQkfAEBSwgcAkJTwAQAkJXwAAEkJHwBAUsIHAJCU8AEAJCV8AABJCR8AQFLCBwCQlPABACQlfAAASQkfAEBSTSPdAUa/juWbIiLipXsXjnBPqKXhbOd62EfqoY+DdSqNZbCGO+bS61M5HbfRULjzAQAkJXwAAEkJHwBAUsIHAJCU8AEAJCV8AABJCR8AQFLCBwCQlPABACQlfAAASQkfAEBSwgcAkJTwAQAkJXwAAEkJHwBAUsIHAJCU8AEAJCV8AABJCR8AQFLCBwCQVM3DR2dnZxQKhbj99ttr3RQAUAdqGj6effbZePDBB+Md73hHLZsBAOpIzcLHgQMH4mMf+1g89NBDceaZZ9aqGQCgztQsfNxyyy2xcOHCuPbaa0+4XE9PT+zfv7/PBACcuppqUelXv/rV2L59ezz77LMDLtvZ2RmrVq2qRTfqXsfyTRER8dK9C0/q+Xo32PEPt556UjmWwY4fRqPRcFyOVB+G2+5gXz8a1vHx5H7no6urK2677bb4yle+EmPGjBlw+RUrVkR3d3d56urqyrtLAMAokvudj23btsXevXtj3rx55bKjR4/Gli1bYu3atdHT0xONjY3l54rFYhSLxby7AQCMUrmHj2uuuSa+//3v9ym78cYb44ILLog77rijT/AAAE4/uYePCRMmxNy5c/uUjR8/PiZPntyvHAA4/fgfTgGApGry1y7Heuqpp1I0AwDUAXc+AICkhA8AICnhAwBISvgAAJISPgCApIQPACAp4QMASEr4AACSEj4AgKSEDwAgKeEDAEhK+AAAkhI+AICkhA8AICnhAwBISvgAAJISPgCApIQPACCpppHuwGjVsXxTRES8dO/CE5adTD2pDTSWFH0cDevheEp9K6lFH6ut74GWS+FE+0alk9nnR4vR1J/RfBycyGD7Pdzx5bF+Krf3qX5OGw19OFnufAAASQkfAEBSwgcAkJTwAQAkJXwAAEkJHwBAUsIHAJCU8AEAJCV8AABJCR8AQFLCBwCQlPABACQlfAAASQkfAEBSwgcAkJTwAQAkJXwAAEkJHwBAUsIHAJCU8AEAJJV7+Ojs7Ix3vetdMWHChJg6dWr81m/9Vrzwwgt5NwMA1Kncw8fTTz8dt9xyS3z3u9+NzZs3x5tvvhnXX399HDx4MO+mAIA61JR3hf/8z//c5/H69etj6tSpsW3btviVX/mVvJsDAOpM7uHjWN3d3RERMWnSpKrP9/T0RE9PT/nx/v37a90lAGAE1TR8ZFkWy5Yti6uuuirmzp1bdZnOzs5YtWpVLbtREx3LN/V5/NK9C3Op73j1HNveUF+fR3/ybmOgtgcqG0wdJ9vXk21vOG0OVHdeY8mzf7XcLifaB0tGciyDre+lexeesO6R3L7DOe/kdT4YTj2D7f9Adee13GhYn/Wipn/t8qlPfSr+8z//Mx599NHjLrNixYro7u4uT11dXbXsEgAwwmp25+PTn/50PPHEE7Fly5Z429vedtzlisViFIvFWnUDABhlcg8fWZbFpz/96XjsscfiqaeeilmzZuXdBABQx3IPH7fcckts2LAh/vEf/zEmTJgQe/bsiYiItra2GDt2bN7NAQB1JvfvfKxbty66u7tj/vz5cfbZZ5enjRs35t0UAFCHavKxCwDA8fhtFwAgKeEDAEhK+AAAkhI+AICkhA8AICnhAwBISvgAAJISPgCApIQPACAp4QMASEr4AACSEj4AgKSEDwAgKeEDAEhK+AAAkhI+AICkhA8AICnhAwBIqmmkOzCSOpZv6vP4pXsXDmq5gep76d6FVV9Trb3BLDcUQ+3D8V472HZOpg+DqaPWhjLWY19TMtA6HkrdeRnsPp1HG8Otu1o9gy2rVs/JtD0c1eoYaJ8/mT4M9nga6LVD7ctwjo1qZSdT31C2eV7nk5OpO6/z6mDqHew6GcoxNBLc+QAAkhI+AICkhA8AICnhAwBISvgAAJISPgCApIQPACAp4QMASEr4AACSEj4AgKSEDwAgKeEDAEhK+AAAkhI+AICkhA8AICnhAwBISvgAAJISPgCApIQPACAp4QMASKpm4eOBBx6IWbNmxZgxY2LevHnxrW99q1ZNAQB1pCbhY+PGjXH77bfHnXfeGc8991y8733viwULFsSuXbtq0RwAUEdqEj7uv//++IM/+IO46aab4sILL4w1a9bEjBkzYt26dbVoDgCoI015V3j48OHYtm1bLF++vE/59ddfH9/5znf6Ld/T0xM9PT3lx93d3RERsX///ry7FhERvT2HyvWX5kvyKqtl3dUYi7EYS/q6qzEWY6mXsdTiGluqM8uygRfOcrZ79+4sIrJvf/vbfcrvueee7Lzzzuu3/F133ZVFhMlkMplMplNg6urqGjAr1OwLp4VCoc/jLMv6lUVErFixIrq7u8vTvn374sc//nG89tprfcrzmrq6uiIioqurqzz/3//937mW1bLu1O3Va93GMjrrNpbRWbexnJ515319fe2116Krqyva29tjILl/7HLWWWdFY2Nj7Nmzp0/53r17Y9q0af2WLxaLUSwW+5RNnDgx727109raWp6fMGFCrmW1rDt1e/Vat7GMzrqNZXTWbSynZ92Vj/PS1tY2qOVyv/PR0tIS8+bNi82bN/cp37x5c1xxxRV5NwcA1Jnc73xERCxbtiwWL14cl19+ebz3ve+NBx98MHbt2hU333xzLZoDAOpITcLHhz/84fjZz34Wd999d7z66qsxd+7c+Kd/+qc499xza9HckBSLxbjrrrvKH/Xcdddd0drammtZLetO3V691m0so7NuYxmddRvL6Vv3SClk2WD+JgYAIB9+2wUASEr4AACSEj4AgKSEDwAgqZr8tUsKDzzwQPzlX/5lvPrqqzF16tQ4cuRI/PSnP42Ghrfy1JgxY+LAgQPR29s7wj0FgNGlsbExsiwb9DWytbU1PvzhD8f9998fZ5xxxrDbr8s7Hxs3bozbb7897rzzzli9enXs3r07fvKTn0RExPve9744evRo+Qduqv2X7gCMXk1Ng3tfXPqfO4ersbExl3pGm9L1r7GxMcaPHx+NjY3R1NQUU6ZMid7e3ujt7Y3m5uaIiDj//POjsbExVq5cGc8880w89thjMW7cuLjgggvinHPOic2bN8cPfvCD+MQnPpFP53L5NbnE3v3ud2c333xzn/kxY8Zkl112Wdbb25tFRNbe3p6NGTMma2pqyj73uc9lEZG1tbVlY8aM6fMDOK2trf1+FGfChAn9yhoaGvo8bmxsHPDHdY5t69hp3Lhx/crGjh3br2z8+PHl+XPOOSeLiKypqalcNnv27PJYfumXfqnf6wfq60D9jIispaXlpH5gqFgsDnuZQqFQnr/uuuv6PX/ttdf2G+uUKVOOO4a2trasubm53/Nz5szpt63b29vL9Zb6UVpfleuktN3a2tr69PfYtiu3RWm7Ve5jpf2x2r5Ruc1L4ysUCuX9o3I9fvzjH88iouo4j63reFO146Ctre2k9oPBtDeY11Tbl489NgfzmmrT8dbV8aalS5f2K7vyyiv7lc2bN6+8/Sv3jdL2quxfqQ+VYyqth8bGxqrHSrX9rdpYhjq+atugpaWlXDZv3rxyX0tlV1xxRb9+TZkypTzGUlnl+ErrpvI8N3fu3CzireOq9Jqzzz67X/8uvPDCfmXHHlcRkc2cObNfe5MmTSrX/dGPfrRcVlru0ksvLT9fWneV66M0hmuuuaa8XUrLHe84qbZPV9t+A03V9unJkyf3K/vABz5Qnr/jjjuyiMiWLFlS9fx41llnZRGRbd68OcuyLPvSl76UTZ06NZsyZUp29913Z1mWZc8991wWEdmPfvSjYV/H6y589PT0ZI2NjdnXvva18vzGjRuzQqGQXXTRRdkPf/jDLCKyWbNmlTfqP/zDP5R37mM3dLUL72AuxnlM1U4k1XbOgS7OlReJagdotTorL5wns/OfaDrjjDP61T3QBeJEU+UJ4bbbbutXXymQnMwFrnKqdnIeTr/zniq3U+XJ7ZJLLskifnHyiIjsL/7iL7KI4e3L1U5wgwmTp8u0ePHifmXVQnq1NziV27Lyolsqr7aeBwpRpf23MihX7ielssr2hrr9r7rqqvL8DTfckEX03cfOO++8k16flevkZIJS3tNgx/KJT3yiT9A6diyppmr72fvf//7y/Ic+9KEs4q2AXHoTW22aNm1aNn369OzCCy/MJk6cmDU0NGS7du3KsizL/ud//ieLiGz9+vXDvpbX3ccuP/3pT+Po0aMxbdq08nyxWIwsy+L111+PF154ISIiDh06FNn////Turu7IyLi8OHD5bKSN998s99HM2+++WaCkUT09PT0Kzt69OgJlyt9p6X0b0TE66+/HhFv3aqsdvuwNJ7K18yYMaM8f+w6Ga4DBw70q3uwnytWu936f//3f+X5hx9+uF99zz//fERU326VY6402I/jSrckR4PK7VTapyMiduzYERF9951nnnkmIvruO0Mdy0D74umu9FFvpWrr7PDhw+X50v5YuS0rf4irVD7Yc0OlI0eOlJc79txXWffBgwdPWE/JpEmT+pWV9quIiCeeeCIi+o7vxRdfHFTd1VT+oOj48eNPup68VBvL5MmTqy5bWrelfaIWP9g2kNJXDSr913/9V3n+3/7t3yIi4tvf/nbs3r07IiLGjh3b7zWNjY3x9a9/PWbNmhWvvfZazJ49O6ZNmxb79u2LP/3TP42IiFdffXXY/a278FFSefEozVcrG0mNjY3lA2qwn2EOpFo9pQO1oaHhhCeWygt25QX9WMf7/HPu3LknfP7Y/pyMgYLfz3/+835l1Q66akr9am5urhq4SifvSkO92FYLO8cLQHk5dOhQv7Knn346Ivpe5KqNj7eczDaqdo657LLL+pX9+q//enm+WgCsdsxW+y7DtddeW56//PLLj9uHStUugoM9F53oHBHxi2Oj8rxSbT2+853vHFR7+/btK8+/9tprg3pNXkrbpbL/1c5z1fpV7Y1VtV92HYk3Mnv37i3PV74pLHnjjTf6PD7rrLPilVdeiY6Ojvibv/mbiIjYvXt3jBs3LqZPn14OInl8R6buwsdZZ50VjY2NsWfPnvL8G2+8EYVCIc4444w4//zzI+KtRFc6MEs7QktLS7+Dtampqd+FKK+gEPGLHbPaxa7a/61fre3KPpfeZVTu8KW6q11cGhoaYvbs2f3Kq53cSjtUtXdYDQ0N5XVb+Xy1/h67Q+ep8l1WyYkCQuV6Kp3kj7d98zigqp1gxowZM+x6jzV9+vSI+MU31iP67mOl9TScEF5tLENZR9XeVY2GNwXVnMxfxZ1zzjn9yrZu3dqv7Bvf+EZ5vr29PSL6XuSqhfXzzz+/37r6/ve/X56v9s6z2oW/WjAd6A7KYJerdr6pth63b98+qPYq1TqwH6s0lsr+Vxv/0aNH+22XPXv29Ftu165dx21juGbNmlWeLx2jY8aMKe9bLS0tERExbty4uO666yLirWNx4sSJUSwW++xbx16XSnfKduzYERs2bIimpqa49dZbY/fu3fGzn/0sVq5cGT/5yU/69OFk1V34aGlpiXnz5sXmzZvL80899VQUi8Xyio1464JULBajqakpfvjDH0bEWxedYy/4pQ1VqdpJ99iDYTAn0aampvK78mo7crWLYLW2x40bV56/6KKL+i1XOskXCoV+4ysUCvGlL32pX53vfe97+5VdeeWVEfHWWI/tR0tLS3zve9/rU9bY2Fj1JFG5Q1dbT8dewI73A0eld22VJ+cpU6ZERN91V5pvbm4uz5eWq2yrFEKPd2Ib6GQ7mHcug92mEf3XTWW/Ghoaqu5zpbLSHaDe3t7y+qt8Z1MqO95Yj90G1bZTtWOjWqA4Xr2Vd6lK66By3xgoyAz0JqDaR5DVDDbwDDV8VgvwK1euPGG9N910U7ms1K+Ojo5+r+ns7CzPl9ZDZciuDB+l5yvbKa2TaneDm5uby8ueaN1V1lc6B3V0dJTLS8dToVAo1/3+97+/3+vnzJlz3DaOpzIEDPWOQbXtWG2clf0uhflKZ599dr+yc889t99x8eSTT5bnS280qt35qNZ+5b+DXb7yuCoFmjfeeKN8bJXeeBw6dCimTp1afs3s2bPj8OHDsW7dunjooYf6td/Y2Fiub/r06bF+/fpobm4u3+0444wzYuPGjTFmzJhyqBmWYX9rZAR89atfzZqbm7O///u/z/76r/86a2hoyBobG7OmpqbsuuuuK39RqqGhISsUCiPy5R+TaSQn+/zIrOPKL1sPZxpNX3QeylTtL6ROpanyy7yD3SdSTMVisV/bZ555Zp+/lGpqasrOO++87KabbsrGjRtX/lLzNddck0X84gv773nPe7L169dnEW/91d2TTz6ZvfDCC9natWuzsWPHZl/4whdyuY7XZfjIsiz727/92+zcc8/NWlpashkzZmTTp0/PGhsbs5aWlqy5uTlrbW2t2wPYZDKZTKZaTs3NzX2ukWeeeWb2nve8J5syZUrW1NSUtbW1ZYsWLcomTZqUtbS0ZO94xzuyL3/5y7ldwwtZlvOfOgAAnEDdfecDAKhvwgcAkJTwAQAkJXwAAEkJHwBAUsIHAJCU8AEAJCV8AABJCR8AQFLCBwCQlPABACQlfAAASf0/9KSZxFxygKIAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(st,ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current projection is: EPSG 5070\n",
      " Reporjecting to WGS84 (EPSG 4326)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Vito\\miniconda3\\envs\\gee\\Lib\\site-packages\\pyproj\\crs\\crs.py:141: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  in_crs_string = _prepare_from_proj_string(in_crs_string)\n",
      "c:\\Users\\Vito\\miniconda3\\envs\\gee\\Lib\\site-packages\\pyproj\\crs\\crs.py:141: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  in_crs_string = _prepare_from_proj_string(in_crs_string)\n",
      "C:\\Users\\Vito\\AppData\\Local\\Temp\\ipykernel_9712\\1492638990.py:11: FutureWarning: This function is deprecated. See: https://pyproj4.github.io/pyproj/stable/gotchas.html#upgrading-to-pyproj-2-from-pyproj-1\n",
      "  lon, lat = transform(source_projection, target_projection, x, y)\n"
     ]
    }
   ],
   "source": [
    "#getting the center coordinate of all sites\n",
    "epsg_code = gdf.crs.to_epsg()\n",
    "print(f\"The current projection is: EPSG {epsg_code}\\n Reporjecting to WGS84 (EPSG 4326)\")\n",
    "source_projection = Proj(init=f'epsg:{epsg_code}')\n",
    "target_projection = Proj(init='epsg:4326')  # WGS84\n",
    "\n",
    "#epsgcenterlist=[]\n",
    "wgscenterlist=[]\n",
    "for geom in geometry:\n",
    "    x, y = geom.centroid.x, geom.centroid.y\n",
    "    lon, lat = transform(source_projection, target_projection, x, y)\n",
    "    #epsgcenterlist.append([x, y])\n",
    "    wgscenterlist.append([lon, lat])\n",
    "np.save('Data/'+Case+'/masklayers/wgscenterlist.npy', wgscenterlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading NorthDakota NDVI:  21%|██        | 5/24 [1:54:34<6:41:37, 1268.28s/site]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: download failed for site 81 at year 2015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading NorthDakota NDVI:  50%|█████     | 12/24 [5:00:25<4:41:14, 1406.19s/site]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: download failed for site 88 at year 2008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading NorthDakota NDVI: 100%|██████████| 24/24 [10:22:54<00:00, 1557.28s/site] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed sites: ['81_2015', '88_2008']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#MODIS NDVI download\n",
    "NDVI_failed_sites = []\n",
    "step=1\n",
    "for i in tqdm(np.arange(77,101,1),'Downloading '+Case+' NDVI', unit='site'):\n",
    "    try:\n",
    "        templist=[]\n",
    "        for yt in np.arange(2006,2023,step):\n",
    "            templist.append(getsiteNDVI(roiall, NDVICollection, i, f'{yt}-01-01', f'{yt+step}-01-01', 30))\n",
    "        St=pd.concat(templist, axis=0, join='inner')\n",
    "        St.to_hdf(sites_pth+f'/Site{i:03}_NBARint.h5', key='df', mode='w')\n",
    "    except Exception as e:\n",
    "        print(f\"Error: download failed for site {i} at year {yt}\")\n",
    "        NDVI_failed_sites.append(i)  \n",
    "print(f\"Failed sites: {NDVI_failed_sites}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading NorthDakota NDVI: 100%|██████████| 48/48 [21:12:32<00:00, 1590.69s/site]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed sites: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#MODIS NDVI download\n",
    "NDVI_failed_sites = []\n",
    "step=1\n",
    "for i in tqdm(np.arange(102,150,1),'Downloading '+Case+' NDVI', unit='site'):\n",
    "    try:\n",
    "        templist=[]\n",
    "        for yt in np.arange(2006,2023,step):\n",
    "            templist.append(getsiteNDVI(roiall, NDVICollection, i, f'{yt}-01-01', f'{yt+step}-01-01', 30))\n",
    "        St=pd.concat(templist, axis=0, join='inner')\n",
    "        St.to_hdf(sites_pth+f'/Site{i:03}_NBARint.h5', key='df', mode='w')\n",
    "    except Exception as e:\n",
    "        print(f\"Error: download failed for site {i} at year {yt}\")\n",
    "        NDVI_failed_sites.append(i)  \n",
    "print(f\"Failed sites: {NDVI_failed_sites}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading NorthDakota NDVI: 100%|██████████| 30/30 [12:19:43<00:00, 1479.46s/site]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed sites: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#MODIS NDVI download\n",
    "NDVI_failed_sites = []\n",
    "step=1\n",
    "for i in tqdm(np.arange(170,num_sites,1),'Downloading '+Case+' NDVI', unit='site'):\n",
    "    try:\n",
    "        templist=[]\n",
    "        for yt in np.arange(2006,2023,step):\n",
    "            templist.append(getsiteNDVI(roiall, NDVICollection, i, f'{yt}-01-01', f'{yt+step}-01-01', 30))\n",
    "        St=pd.concat(templist, axis=0, join='inner')\n",
    "        St.to_hdf(sites_pth+f'/Site{i:03}_NBARint.h5', key='df', mode='w')\n",
    "    except Exception as e:\n",
    "        print(f\"Error: download failed for site {i} at year {yt}\")\n",
    "        NDVI_failed_sites.append(i)  \n",
    "print(f\"Failed sites: {NDVI_failed_sites}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NDVI download\n",
    "failed_sites = []\n",
    "for i in [12,26,27,73,89]:\n",
    "    templist=[]\n",
    "    for yt in np.arange(2006,2023,1):\n",
    "        try:\n",
    "            templist.append(getsiteNDVI(roiall, NDVICollection, i, f'{yt}-01-01', f'{yt}-04-01', 30))\n",
    "            templist.append(getsiteNDVI(roiall, NDVICollection, i, f'{yt}-04-01', f'{yt}-07-01', 30))\n",
    "            templist.append(getsiteNDVI(roiall, NDVICollection, i, f'{yt}-07-01', f'{yt}-10-01', 30))\n",
    "            templist.append(getsiteNDVI(roiall, NDVICollection, i, f'{yt}-10-01', f'{yt+1}-01-01', 30))\n",
    "        except Exception as e:\n",
    "            print(f\"Error: download failed for site {i} year {yt}\")\n",
    "            failed_sites.append(f'{i}_{yt}')\n",
    "    St=pd.concat(templist, axis=0, join='inner')\n",
    "    St.to_hdf(sites_pth+f'/Site{i:03}_NBARint.h5', key='df', mode='w')\n",
    "print(f\"Failed sites: {failed_sites}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "problem for season download\n",
    "site 12\n",
    "26\n",
    "73\n",
    "79\n",
    "89\n",
    "works after spliting in 5 years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: download failed for site 12\n"
     ]
    }
   ],
   "source": [
    "#NDSI season download\n",
    "NDSI_failed_sites = []\n",
    "for i in tqdm(np.arange(0,num_sites,1),'Downloading '+Case+' NDSI', unit='site'):\n",
    "    try:\n",
    "        d1,snow1,lst1=Season(roiall, i, 2006, 2013)\n",
    "        d2,snow2,lst2=Season(roiall, i, 2013, 2023)\n",
    "        St1=pd.concat([d1,d2], axis=0, join='inner')\n",
    "        St1.to_hdf(season_pth+f'/Site{i:03}_season_day.h5', key='df', mode='w')\n",
    "        St2=pd.concat([snow1,snow2], axis=0, join='inner')\n",
    "        St2.to_hdf(season_pth+f'/Site{i:03}_snow_ts.h5', key='df', mode='w') \n",
    "        St3=pd.concat([lst1,lst2], axis=0, join='inner')\n",
    "        St3.to_hdf(season_pth+f'/Site{i:03}_lst_ts.h5', key='df', mode='w') \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: download failed for site {i}\")\n",
    "        NDSI_failed_sites.append(i)\n",
    "print(f\"Failed sites: {NDSI_failed_sites}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "26\n",
      "73\n",
      "79\n",
      "89\n",
      "Failed sites: []\n"
     ]
    }
   ],
   "source": [
    "#season download splited\n",
    "run=0\n",
    "while len(NDSI_failed_sites)>0:\n",
    "    print(f\"Retrying sites: {NDSI_failed_sites}\")\n",
    "    new_NDSI_failed_sites = []\n",
    "    for i in tqdm(NDSI_failed_sites,'Downloading '+Case+' NDSI', unit='site'):\n",
    "        try:\n",
    "            d1,snow1,lst1=Season(roiall, i, 2006, 2010)\n",
    "            d2,snow2,lst2=Season(roiall, i, 2010, 2015)\n",
    "            d3,snow3,lst3=Season(roiall, i, 2015, 2020)\n",
    "            d4,snow4,lst4=Season(roiall, i, 2020, 2023)\n",
    "            \n",
    "            St1=pd.concat([d1,d2,d3,d4], axis=0, join='inner')\n",
    "            St1.to_hdf(season_pth+f'/Site{i:03}_season_day.h5', key='df', mode='w')\n",
    "            St2=pd.concat([snow1,snow2,snow3,snow4], axis=0, join='inner')\n",
    "            St2.to_hdf(season_pth+f'/Site{i:03}_snow_ts.h5', key='df', mode='w') \n",
    "            St3=pd.concat([lst1,lst2,lst3,lst4], axis=0, join='inner')\n",
    "            St3.to_hdf(season_pth+f'/Site{i:03}_lst_ts.h5', key='df', mode='w') \n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error: download failed for site {i}\")\n",
    "            new_NDSI_failed_sites.append(i)\n",
    "    if NDSI_failed_sites == new_NDSI_failed_sites:\n",
    "        print(f\"Error: number of failed sites not reducing. May need further splitting\")\n",
    "        break\n",
    "    NDSI_failed_sites = new_NDSI_failed_sites\n",
    "    print(f\"Failed sites: {failed_sites}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: download failed for site 121\n",
      "Failed sites: [121]\n"
     ]
    }
   ],
   "source": [
    "#site cdl donwload\n",
    "cdl_failed_sites = []\n",
    "for i in tqdm(np.arange(0,num_sites,1),'Downloading '+Case+' CDL', unit='site'):\n",
    "    try:\n",
    "        temp=cdldf(cdl, i)\n",
    "        temp.to_hdf(cdl_pth+f'/Site{i:03}_cdl.h5', key='df', mode='w') \n",
    "    except Exception as e:\n",
    "        print(f\"Error: download failed for site {i}\")\n",
    "        cdl_failed_sites.append(i)\n",
    "print(f\"First run failed sites: {cdl_failed_sites}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run=0\n",
    "while len(cdl_failed_sites)>0:\n",
    "    new_cdl_failed_sites = []\n",
    "    for i in tqdm(cdl_failed_sites,'Downloading '+Case+' CDL', unit='site'):\n",
    "        try:\n",
    "            temp=cdldf(cdl, i)\n",
    "            temp.to_hdf(cdl_pth+f'/Site{i:03}_cdl.h5', key='df', mode='w') \n",
    "        except Exception as e:\n",
    "            print(f\"Error: download failed for site {i}\")\n",
    "            new_cdl_failed_sites.append(i)\n",
    "    cdl_failed_sites = new_cdl_failed_sites\n",
    "    print(f\"Failed sites run {run}: {cdl_failed_sites}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed sites: []\n"
     ]
    }
   ],
   "source": [
    "#sentinel 1 donwload\n",
    "failed_sites = []\n",
    "for i in tqdm(np.arange(0,num_sites,1),'Downloading '+Case+' Sentinel 1', unit='site'):\n",
    "    templist=[]\n",
    "    try:\n",
    "        df = s1data(2015, 2023, i, direction = 'Decending', orbit = None, dataset='LOG')\n",
    "        df.to_hdf(sentinel1_pth+f'/Site{i:03}_db.h5', key='df', mode='w') \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: download failed for site {i}\")\n",
    "        failed_sites.append(i)\n",
    "print(f\"Failed sites: {failed_sites}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gee",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
